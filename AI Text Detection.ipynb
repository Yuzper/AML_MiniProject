{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b61c49-6a02-4cb0-bc98-d8047c811f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "pandas is already installed.\n",
      "matplotlib is already installed.\n",
      "sklearn is already installed.\n",
      "tensorflow is already installed.\n",
      "datasets is already installed.\n",
      "transformers is already installed.\n",
      "tf-keras not found. Installing...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Dict of required packages\n",
    "required_packages = {\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"sklearn\": \"scikit-learn\",\n",
    "    \"tensorflow\": \"tensorflow\",\n",
    "    \"datasets\": \"datasets\",\n",
    "    \"transformers\": \"transformers\",\n",
    "    \"tf-keras\": \"tf-keras\"\n",
    "}\n",
    "\n",
    "def install_and_import(pkg_name, install_name=None):\n",
    "    install_name = install_name or pkg_name\n",
    "    try:\n",
    "        importlib.import_module(pkg_name)\n",
    "        print(f\"{pkg_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{pkg_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", install_name])\n",
    "\n",
    "# Loop and ensure all are installed\n",
    "for pkg, pip_name in required_packages.items():\n",
    "    install_and_import(pkg, pip_name)\n",
    "\n",
    "####################################################################################################################################\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import TFBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb52b26c-4d94-4225-a759-75f34a70c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds defined.\n"
     ]
    }
   ],
   "source": [
    "# Set random states for reproducability\n",
    "RandomState = 42\n",
    "random.seed(RandomState)\n",
    "np.random.seed(RandomState)\n",
    "skl_rand = check_random_state(RandomState)\n",
    "tf.random.set_seed(RandomState)\n",
    "\n",
    "print(\"Random seeds defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186ad94-a09a-473a-816a-eed71cbef135",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- Dataset Loading/Choosing âœ…\n",
    "- Dataset Cleaning\n",
    "- Exploratory Data Analysis\n",
    "- Baseline (TBD)\n",
    "- BERT fine-tuning to classify text\n",
    "- Error Analysis / Robustness Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b940e4-3dec-4b17-85f0-1640aa493b87",
   "metadata": {},
   "source": [
    "# Dataset Loading/Choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe024d4-b838-464c-9534-e1f50d100ead",
   "metadata": {},
   "source": [
    "- LLM - Detect AI Generated Text Dataset (28k essays)\n",
    "https://www.kaggle.com/datasets/sunilthite/llm-detect-ai-generated-text-dataset.\n",
    "- Dataset Card for AI Text Dectection Pile (1.4mil essays)\n",
    "https://huggingface.co/datasets/artem9k/ai-text-detection-pile\n",
    "- Raid (10+mil essays from 10 genres) https://github.com/liamdugan/raid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b977d1-2d8e-4d03-97da-a57ae83965e2",
   "metadata": {},
   "source": [
    "# Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3e571-6241-4044-bb03-1a97bd5e03d0",
   "metadata": {},
   "source": [
    "- Lowercasing (optional with BERT since it's often case-aware depending on the model)\n",
    "- Removing HTML tags, extra spaces\n",
    "- Filtering by length (exclude very short texts)\n",
    "- Removing duplicates\n",
    "- Language detection if you need only English\n",
    "- Where possible tag which model AI text is from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0834a-d1aa-466f-9d6a-56a225e6dc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042f0eea072a4a24b04d5e26ebfe2a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Different selection of data\n",
    "# Takes a long time to load first time around...\n",
    "data_all = load_dataset(\"liamdugan/raid\", \"raid\")\n",
    "train_data = data_all[\"train\"]\n",
    "train_data_subset = train_data.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d0ffd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'adv_source_id': Value(dtype='string', id=None),\n",
       " 'source_id': Value(dtype='string', id=None),\n",
       " 'model': Value(dtype='string', id=None),\n",
       " 'decoding': Value(dtype='string', id=None),\n",
       " 'repetition_penalty': Value(dtype='string', id=None),\n",
       " 'attack': Value(dtype='string', id=None),\n",
       " 'domain': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'prompt': Value(dtype='string', id=None),\n",
       " 'generation': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_subset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ba620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict\n",
    "\n",
    "# Combine title + generation into one for training\n",
    "def prepare_text(dataset: Dict) -> Dict:\n",
    "    if dataset[\"title\"] is None or dataset[\"generation\"] is None:\n",
    "        return None\n",
    "    dataset[\"text\"] = dataset[\"title\"].strip() + \" \" + dataset[\"generation\"].strip()\n",
    "    return dataset\n",
    "\n",
    "# Encode binary labels\n",
    "def encode_label(dataset: Dict, label_map={\"human\": 0, \"machine\": 1}) -> Dict:\n",
    "    dataset[\"label\"] = label_map.get(dataset[\"model\"], -1)\n",
    "    return dataset\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_example(dataset: Dict, tokenizer) -> Dict:\n",
    "    tokens = tokenizer(\n",
    "        dataset[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"label\"] = dataset[\"label\"]\n",
    "    return tokens\n",
    "\n",
    "# Full \"main\" pipeline\n",
    "def prepare_dataset_for_bert(dataset, tokenizer_name=\"bert-base-uncased\"):\n",
    "    # Filter rows with missing title or generation\n",
    "    dataset = dataset.filter(lambda x: x[\"title\"] is not None and x[\"generation\"] is not None)\n",
    "    dataset = dataset.map(prepare_text)\n",
    "    \n",
    "    # Encode labels\n",
    "    dataset = dataset.map(encode_label)\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    dataset = dataset.map(lambda x: tokenize_example(x, tokenizer), batched=False)\n",
    "\n",
    "    # Set format for tf.data.Dataset\n",
    "    dataset.set_format(type=\"tensorflow\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Convert to tf.data.Dataset\n",
    "    features = {\n",
    "        \"input_ids\": tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
    "        \"attention_mask\": tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
    "    }\n",
    "\n",
    "    tf_dataset = dataset.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=\"label\",\n",
    "        shuffle=True,\n",
    "        batch_size=16,\n",
    "        collate_fn=None\n",
    "    )\n",
    "    \n",
    "    return tf_dataset, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538f6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>human</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>None</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>human</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>None</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>human</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>None</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>human</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  e5e058ce-be2b-459d-af36-32532aaba5ff   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  f95b107b-d176-4af5-90f7-4d0bb20caf93   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  856d8972-9e3d-4544-babc-0fe16f21e04d   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  72c41b8d-0069-4886-b734-a4000ffca286   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  human     None               None   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  human     None               None   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  human     None               None   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human     None               None   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  human     None               None   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...   None   \n",
       "1   none  abstracts  EdgeFlow: Achieving Practical Interactive Segm...   None   \n",
       "2   none  abstracts  Semi-supervised Contrastive Learning for Label...   None   \n",
       "3   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...   None   \n",
       "4   none  abstracts  Attention-Based 3D Seismic Fault Segmentation ...   None   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_data_subset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccdfab-d6fc-442a-beb0-ba0e56275a76",
   "metadata": {},
   "source": [
    "The fetching of data will take some minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a647ddc7-b867-4786-a54c-6ac2581c2147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b0bba75fb441a5b78bf3dc6b1b667e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558e3ec9148c47a2abcfc458977e26b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bdcb90395542e5a9393dd77f841db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17e3ad103364a1c929f16b033da99e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25af2785e224fb2a4104ae704b321c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jespe\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b35735fa89d46399a9827559de0ba8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c346b999c22d419bbc1fe0dac0088da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3929338805421f9e35144388ed9f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee0fc603e4344c9a38ee8561e6b2cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/493 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d7aab8438846fc8f82028350ec2884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 4/31 [==>...........................] - ETA: 12:02 - loss: 0.3544 - accuracy: 0.9844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TFBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m),\n\u001b[0;32m      7\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      8\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_tf_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1802\u001b[0m ):\n\u001b[0;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    870\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    871\u001b[0m   )\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Jespe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Super basic model to validate data cleaning worked.\n",
    "train_tf_dataset, tokenizer = prepare_dataset_for_bert(train_data_subset)\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(train_tf_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2cae23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6e077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08abbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eaecfa-a019-4d9b-920b-b8ef7ba5beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dedf0ea-a543-4ea0-bd9d-deefea08ce06",
   "metadata": {},
   "source": [
    "Possible extra feature engineering, usefullness unsure.\n",
    "BERT embeddings + handcrafted features like:\n",
    "- Average sentence length\n",
    "- N-gram repetition\n",
    "- Ratio of stopwords Then feed that into a LightGBM/XGBoost model to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe890e8-15c1-48a9-8f8b-6fc18186526c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "- Text length distributions\n",
    "- Vocabulary richness (e.g. unique words)\n",
    "- POS tag distribution (maybe AI uses more nouns, fewer adjectives?)\n",
    "- Visualizations: word clouds, frequency plots\n",
    "- Clustering to check for seperability of classes\n",
    "- .\n",
    "- Comparing perplexity charts of AI model text and human text, can help understand the complexity of the task at hand, due to the variety of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd734ac7-4a8b-41b9-a9e2-0d4df78adc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026c671-3066-439e-84e3-4704aa4c016f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc197707-f201-4334-956f-9407a59f6ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ac24b-7fc4-432d-bf6c-cb29f0185e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f028da7-0d04-47d9-b8cb-29baf1198740",
   "metadata": {},
   "source": [
    "# Baseline (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed4054-dcf9-4017-810d-f6366a4f7277",
   "metadata": {},
   "source": [
    "- Basic baseline logistic regression etc (Might not be relevant)\n",
    "- Basic Deep learning artitecture\n",
    "- Base BERT\n",
    "- Maybe a basic baseline and a base BERT to see how much performance BERT adds and how much fine-tuned BERT additionally adds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78583d4d-2092-4719-b170-918f8b1f7fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e246f52-45a6-4234-8961-03999f65db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054f3ea-b3c2-46e3-b31b-4bcbfc92c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d000238b-8464-48b5-822f-9c029dc44695",
   "metadata": {},
   "source": [
    "# BERT fine-tuning to classify text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bf97b-7b78-49ee-94c6-b94adc10f892",
   "metadata": {},
   "source": [
    "- BERT vs RoBERTa vs DistilBERT\n",
    "- RoBERTa often performs better in classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152944cd-2bcb-4bac-a4db-b9ee66db9818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af2f3b-a631-4c45-8cbd-d52411bfa0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add36150-29da-4847-8bd5-7c04163d7483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1477b62-929b-4b20-b833-a280b1c444ab",
   "metadata": {},
   "source": [
    "# Error Analysis / Robustness Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829dd137-e806-4c8b-af04-94669f037fba",
   "metadata": {},
   "source": [
    "- What types of errors does it make confusion matrix?\n",
    "- Is the model biased toward longer/shorter texts?\n",
    "- Attention analysis (using tools like BertViz)\n",
    "- Check if BERT overfits to text length or formatting\n",
    "- Does it misclassify texts on certain topics?\n",
    "- Could it unfairly flag texts written by non-native speakers?\n",
    "- Does it perform better on specific outpurs from specific models?\n",
    "- Small edits (punctuation, synonyms) and how does they affect the model?\n",
    "- Test synonym replacements (e.g., \"happy\" â†’ \"joyful\") with slight paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f6e90-7bb1-4256-9c3e-bce884aaea41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d410c-451a-43e2-92ef-414fc48a956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715e307-299a-4354-ab8d-ee7c29aed543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d66bc6-3d03-455f-8ccb-b5cf79e5a580",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf75932-d9fe-4700-abfe-04ab5500b6dd",
   "metadata": {},
   "source": [
    "- Attention Heatmap (with bertviz or transformers-interpret)\n",
    "- Visualize token importance\n",
    "- SHAP map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4d10b-2190-4c73-b548-c06281947120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2066bf-9995-4734-98c6-bbb653d11c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459c6d0-f57a-4062-ad81-24a2141a09ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "277951af-76de-4751-b998-5717817c8151",
   "metadata": {},
   "source": [
    "# (Option extra if time allows) Own trained text generator, compare its outputs predictions to the ones from other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2764aa2-5217-4e66-8405-3e8391f5f2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee3f47-51e9-4ae0-8fa2-7c1cb1d647cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd5d30-84ec-4c73-a485-b307e16e5558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fed95-cfab-4303-96e2-5bed8d1ef414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed6a00-f7a0-4659-a820-0d5ed8ea83f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c95665-c453-421d-8bf5-59e0ddfa6ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccebfe-a932-4e7c-8c97-5f1bb0b67396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff2090-2543-42c6-aa6d-cae5bebb20c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
