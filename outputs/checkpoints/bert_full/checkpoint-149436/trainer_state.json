{
  "best_global_step": 74718,
  "best_metric": 0.9840065312240691,
  "best_model_checkpoint": "outputs/checkpoints/bert_full/checkpoint-74718",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 149436,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006691827939720014,
      "grad_norm": 0.04796206206083298,
      "learning_rate": 1.9973286222864637e-05,
      "loss": 0.1402,
      "step": 500
    },
    {
      "epoch": 0.013383655879440027,
      "grad_norm": 0.3119257390499115,
      "learning_rate": 1.9946625980352794e-05,
      "loss": 0.106,
      "step": 1000
    },
    {
      "epoch": 0.020075483819160043,
      "grad_norm": 0.024958932772278786,
      "learning_rate": 1.9919858668593914e-05,
      "loss": 0.1011,
      "step": 1500
    },
    {
      "epoch": 0.026767311758880055,
      "grad_norm": 0.013468916527926922,
      "learning_rate": 1.9893091356835034e-05,
      "loss": 0.0745,
      "step": 2000
    },
    {
      "epoch": 0.03345913969860007,
      "grad_norm": 9.817083358764648,
      "learning_rate": 1.9866377579699673e-05,
      "loss": 0.0799,
      "step": 2500
    },
    {
      "epoch": 0.04015096763832009,
      "grad_norm": 0.03618215397000313,
      "learning_rate": 1.9839663802564312e-05,
      "loss": 0.0617,
      "step": 3000
    },
    {
      "epoch": 0.046842795578040095,
      "grad_norm": 0.005474668461829424,
      "learning_rate": 1.9812896490805432e-05,
      "loss": 0.062,
      "step": 3500
    },
    {
      "epoch": 0.05353462351776011,
      "grad_norm": 0.14690949022769928,
      "learning_rate": 1.978612917904655e-05,
      "loss": 0.0646,
      "step": 4000
    },
    {
      "epoch": 0.060226451457480124,
      "grad_norm": 5.731198310852051,
      "learning_rate": 1.975936186728767e-05,
      "loss": 0.0507,
      "step": 4500
    },
    {
      "epoch": 0.06691827939720014,
      "grad_norm": 0.32261762022972107,
      "learning_rate": 1.973259455552879e-05,
      "loss": 0.0625,
      "step": 5000
    },
    {
      "epoch": 0.07361010733692015,
      "grad_norm": 0.041659314185380936,
      "learning_rate": 1.970582724376991e-05,
      "loss": 0.0549,
      "step": 5500
    },
    {
      "epoch": 0.08030193527664017,
      "grad_norm": 0.0012330882018432021,
      "learning_rate": 1.967905993201103e-05,
      "loss": 0.061,
      "step": 6000
    },
    {
      "epoch": 0.08699376321636018,
      "grad_norm": 0.0013861780753359199,
      "learning_rate": 1.965229262025215e-05,
      "loss": 0.064,
      "step": 6500
    },
    {
      "epoch": 0.09368559115608019,
      "grad_norm": 0.06197873130440712,
      "learning_rate": 1.962557884311679e-05,
      "loss": 0.0539,
      "step": 7000
    },
    {
      "epoch": 0.10037741909580021,
      "grad_norm": 5.5167646408081055,
      "learning_rate": 1.9598811531357906e-05,
      "loss": 0.0516,
      "step": 7500
    },
    {
      "epoch": 0.10706924703552022,
      "grad_norm": 34.947322845458984,
      "learning_rate": 1.9572044219599026e-05,
      "loss": 0.0509,
      "step": 8000
    },
    {
      "epoch": 0.11376107497524024,
      "grad_norm": 0.034620121121406555,
      "learning_rate": 1.9545276907840147e-05,
      "loss": 0.0614,
      "step": 8500
    },
    {
      "epoch": 0.12045290291496025,
      "grad_norm": 0.07794958353042603,
      "learning_rate": 1.9518563130704785e-05,
      "loss": 0.0588,
      "step": 9000
    },
    {
      "epoch": 0.12714473085468025,
      "grad_norm": 0.012425108812749386,
      "learning_rate": 1.9491795818945906e-05,
      "loss": 0.0533,
      "step": 9500
    },
    {
      "epoch": 0.1338365587944003,
      "grad_norm": 0.002988613909110427,
      "learning_rate": 1.9465028507187026e-05,
      "loss": 0.0591,
      "step": 10000
    },
    {
      "epoch": 0.1405283867341203,
      "grad_norm": 0.008023136295378208,
      "learning_rate": 1.9438261195428146e-05,
      "loss": 0.0511,
      "step": 10500
    },
    {
      "epoch": 0.1472202146738403,
      "grad_norm": 0.0007601860561408103,
      "learning_rate": 1.9411493883669266e-05,
      "loss": 0.0384,
      "step": 11000
    },
    {
      "epoch": 0.1539120426135603,
      "grad_norm": 0.04046809300780296,
      "learning_rate": 1.9384780106533902e-05,
      "loss": 0.0456,
      "step": 11500
    },
    {
      "epoch": 0.16060387055328035,
      "grad_norm": 0.1691139191389084,
      "learning_rate": 1.9358012794775022e-05,
      "loss": 0.0511,
      "step": 12000
    },
    {
      "epoch": 0.16729569849300036,
      "grad_norm": 0.09799393266439438,
      "learning_rate": 1.9331245483016142e-05,
      "loss": 0.0525,
      "step": 12500
    },
    {
      "epoch": 0.17398752643272036,
      "grad_norm": 0.10024482756853104,
      "learning_rate": 1.9304478171257263e-05,
      "loss": 0.0584,
      "step": 13000
    },
    {
      "epoch": 0.18067935437244037,
      "grad_norm": 0.004843780770897865,
      "learning_rate": 1.9277710859498383e-05,
      "loss": 0.0531,
      "step": 13500
    },
    {
      "epoch": 0.18737118231216038,
      "grad_norm": 0.0018327045254409313,
      "learning_rate": 1.9250943547739503e-05,
      "loss": 0.0462,
      "step": 14000
    },
    {
      "epoch": 0.1940630102518804,
      "grad_norm": 0.39669570326805115,
      "learning_rate": 1.9224176235980624e-05,
      "loss": 0.0449,
      "step": 14500
    },
    {
      "epoch": 0.20075483819160042,
      "grad_norm": 0.005839720834046602,
      "learning_rate": 1.919746245884526e-05,
      "loss": 0.0407,
      "step": 15000
    },
    {
      "epoch": 0.20744666613132043,
      "grad_norm": 0.008322926238179207,
      "learning_rate": 1.917069514708638e-05,
      "loss": 0.0481,
      "step": 15500
    },
    {
      "epoch": 0.21413849407104044,
      "grad_norm": 58.176979064941406,
      "learning_rate": 1.91439278353275e-05,
      "loss": 0.0372,
      "step": 16000
    },
    {
      "epoch": 0.22083032201076047,
      "grad_norm": 0.03908105567097664,
      "learning_rate": 1.9117214058192138e-05,
      "loss": 0.0481,
      "step": 16500
    },
    {
      "epoch": 0.22752214995048048,
      "grad_norm": 0.055949293076992035,
      "learning_rate": 1.909044674643326e-05,
      "loss": 0.0484,
      "step": 17000
    },
    {
      "epoch": 0.2342139778902005,
      "grad_norm": 0.37154585123062134,
      "learning_rate": 1.906367943467438e-05,
      "loss": 0.0481,
      "step": 17500
    },
    {
      "epoch": 0.2409058058299205,
      "grad_norm": 6.115246772766113,
      "learning_rate": 1.9036965657539014e-05,
      "loss": 0.0607,
      "step": 18000
    },
    {
      "epoch": 0.2475976337696405,
      "grad_norm": 0.0021434242371469736,
      "learning_rate": 1.9010198345780134e-05,
      "loss": 0.0491,
      "step": 18500
    },
    {
      "epoch": 0.2542894617093605,
      "grad_norm": 0.0011307507520541549,
      "learning_rate": 1.8983431034021255e-05,
      "loss": 0.0406,
      "step": 19000
    },
    {
      "epoch": 0.2609812896490805,
      "grad_norm": 0.029870424419641495,
      "learning_rate": 1.8956663722262375e-05,
      "loss": 0.041,
      "step": 19500
    },
    {
      "epoch": 0.2676731175888006,
      "grad_norm": 0.15462139248847961,
      "learning_rate": 1.8929896410503495e-05,
      "loss": 0.0446,
      "step": 20000
    },
    {
      "epoch": 0.2743649455285206,
      "grad_norm": 21.894237518310547,
      "learning_rate": 1.8903182633368134e-05,
      "loss": 0.0418,
      "step": 20500
    },
    {
      "epoch": 0.2810567734682406,
      "grad_norm": 0.0015144560020416975,
      "learning_rate": 1.887641532160925e-05,
      "loss": 0.0508,
      "step": 21000
    },
    {
      "epoch": 0.2877486014079606,
      "grad_norm": 0.05776594951748848,
      "learning_rate": 1.884964800985037e-05,
      "loss": 0.0427,
      "step": 21500
    },
    {
      "epoch": 0.2944404293476806,
      "grad_norm": 0.04988189786672592,
      "learning_rate": 1.882288069809149e-05,
      "loss": 0.0347,
      "step": 22000
    },
    {
      "epoch": 0.3011322572874006,
      "grad_norm": 0.030231794342398643,
      "learning_rate": 1.879611338633261e-05,
      "loss": 0.0503,
      "step": 22500
    },
    {
      "epoch": 0.3078240852271206,
      "grad_norm": 0.10336820036172867,
      "learning_rate": 1.8769346074573732e-05,
      "loss": 0.0478,
      "step": 23000
    },
    {
      "epoch": 0.31451591316684063,
      "grad_norm": 0.00644893292337656,
      "learning_rate": 1.8742578762814852e-05,
      "loss": 0.0366,
      "step": 23500
    },
    {
      "epoch": 0.3212077411065607,
      "grad_norm": 0.05720679089426994,
      "learning_rate": 1.8715811451055973e-05,
      "loss": 0.0446,
      "step": 24000
    },
    {
      "epoch": 0.3278995690462807,
      "grad_norm": 0.016544243320822716,
      "learning_rate": 1.8689097673920608e-05,
      "loss": 0.0364,
      "step": 24500
    },
    {
      "epoch": 0.3345913969860007,
      "grad_norm": 0.11171264946460724,
      "learning_rate": 1.8662330362161728e-05,
      "loss": 0.0426,
      "step": 25000
    },
    {
      "epoch": 0.3412832249257207,
      "grad_norm": 0.00344551051966846,
      "learning_rate": 1.863556305040285e-05,
      "loss": 0.0387,
      "step": 25500
    },
    {
      "epoch": 0.3479750528654407,
      "grad_norm": 0.01038755290210247,
      "learning_rate": 1.860879573864397e-05,
      "loss": 0.0307,
      "step": 26000
    },
    {
      "epoch": 0.35466688080516073,
      "grad_norm": 0.01718481257557869,
      "learning_rate": 1.858202842688509e-05,
      "loss": 0.04,
      "step": 26500
    },
    {
      "epoch": 0.36135870874488074,
      "grad_norm": 0.011422321200370789,
      "learning_rate": 1.855526111512621e-05,
      "loss": 0.0401,
      "step": 27000
    },
    {
      "epoch": 0.36805053668460075,
      "grad_norm": 0.10280390083789825,
      "learning_rate": 1.852849380336733e-05,
      "loss": 0.0378,
      "step": 27500
    },
    {
      "epoch": 0.37474236462432076,
      "grad_norm": 0.00888754427433014,
      "learning_rate": 1.850172649160845e-05,
      "loss": 0.0297,
      "step": 28000
    },
    {
      "epoch": 0.3814341925640408,
      "grad_norm": 0.002325992565602064,
      "learning_rate": 1.8475012714473085e-05,
      "loss": 0.0437,
      "step": 28500
    },
    {
      "epoch": 0.3881260205037608,
      "grad_norm": 0.001831004861742258,
      "learning_rate": 1.8448245402714205e-05,
      "loss": 0.0411,
      "step": 29000
    },
    {
      "epoch": 0.39481784844348083,
      "grad_norm": 0.0030924235470592976,
      "learning_rate": 1.8421478090955326e-05,
      "loss": 0.0463,
      "step": 29500
    },
    {
      "epoch": 0.40150967638320084,
      "grad_norm": 0.0014463290572166443,
      "learning_rate": 1.8394710779196446e-05,
      "loss": 0.032,
      "step": 30000
    },
    {
      "epoch": 0.40820150432292085,
      "grad_norm": 0.01593749038875103,
      "learning_rate": 1.8367943467437566e-05,
      "loss": 0.0436,
      "step": 30500
    },
    {
      "epoch": 0.41489333226264086,
      "grad_norm": 0.0017704571364447474,
      "learning_rate": 1.8341229690302205e-05,
      "loss": 0.0243,
      "step": 31000
    },
    {
      "epoch": 0.42158516020236086,
      "grad_norm": 0.019163450226187706,
      "learning_rate": 1.8314462378543325e-05,
      "loss": 0.0399,
      "step": 31500
    },
    {
      "epoch": 0.42827698814208087,
      "grad_norm": 0.01753789372742176,
      "learning_rate": 1.8287695066784442e-05,
      "loss": 0.0475,
      "step": 32000
    },
    {
      "epoch": 0.4349688160818009,
      "grad_norm": 0.023159803822636604,
      "learning_rate": 1.8260927755025563e-05,
      "loss": 0.0446,
      "step": 32500
    },
    {
      "epoch": 0.44166064402152094,
      "grad_norm": 0.003751636715605855,
      "learning_rate": 1.8234160443266683e-05,
      "loss": 0.047,
      "step": 33000
    },
    {
      "epoch": 0.44835247196124095,
      "grad_norm": 0.04879254102706909,
      "learning_rate": 1.8207393131507803e-05,
      "loss": 0.0359,
      "step": 33500
    },
    {
      "epoch": 0.45504429990096096,
      "grad_norm": 0.03544230759143829,
      "learning_rate": 1.8180625819748923e-05,
      "loss": 0.0415,
      "step": 34000
    },
    {
      "epoch": 0.46173612784068097,
      "grad_norm": 0.008522383868694305,
      "learning_rate": 1.8153858507990044e-05,
      "loss": 0.0354,
      "step": 34500
    },
    {
      "epoch": 0.468427955780401,
      "grad_norm": 0.1573578417301178,
      "learning_rate": 1.8127091196231164e-05,
      "loss": 0.0388,
      "step": 35000
    },
    {
      "epoch": 0.475119783720121,
      "grad_norm": 0.0008812965243123472,
      "learning_rate": 1.8100323884472284e-05,
      "loss": 0.0321,
      "step": 35500
    },
    {
      "epoch": 0.481811611659841,
      "grad_norm": 0.042298343032598495,
      "learning_rate": 1.8073556572713405e-05,
      "loss": 0.03,
      "step": 36000
    },
    {
      "epoch": 0.488503439599561,
      "grad_norm": 0.004292197059839964,
      "learning_rate": 1.8046789260954525e-05,
      "loss": 0.021,
      "step": 36500
    },
    {
      "epoch": 0.495195267539281,
      "grad_norm": 0.012195637449622154,
      "learning_rate": 1.8020021949195642e-05,
      "loss": 0.0379,
      "step": 37000
    },
    {
      "epoch": 0.501887095479001,
      "grad_norm": 0.003010038286447525,
      "learning_rate": 1.7993254637436762e-05,
      "loss": 0.0356,
      "step": 37500
    },
    {
      "epoch": 0.508578923418721,
      "grad_norm": 0.12932276725769043,
      "learning_rate": 1.796659439492492e-05,
      "loss": 0.037,
      "step": 38000
    },
    {
      "epoch": 0.515270751358441,
      "grad_norm": 0.0034719156101346016,
      "learning_rate": 1.793982708316604e-05,
      "loss": 0.0275,
      "step": 38500
    },
    {
      "epoch": 0.521962579298161,
      "grad_norm": 1.3697911500930786,
      "learning_rate": 1.791305977140716e-05,
      "loss": 0.0359,
      "step": 39000
    },
    {
      "epoch": 0.5286544072378812,
      "grad_norm": 0.03641897067427635,
      "learning_rate": 1.788629245964828e-05,
      "loss": 0.0314,
      "step": 39500
    },
    {
      "epoch": 0.5353462351776012,
      "grad_norm": 0.07218953222036362,
      "learning_rate": 1.7859525147889397e-05,
      "loss": 0.0354,
      "step": 40000
    },
    {
      "epoch": 0.5420380631173212,
      "grad_norm": 0.0013539899373427033,
      "learning_rate": 1.783281137075404e-05,
      "loss": 0.0323,
      "step": 40500
    },
    {
      "epoch": 0.5487298910570412,
      "grad_norm": 0.007444677408784628,
      "learning_rate": 1.7806044058995156e-05,
      "loss": 0.0317,
      "step": 41000
    },
    {
      "epoch": 0.5554217189967612,
      "grad_norm": 0.09833970665931702,
      "learning_rate": 1.7779276747236276e-05,
      "loss": 0.0364,
      "step": 41500
    },
    {
      "epoch": 0.5621135469364812,
      "grad_norm": 0.008745625615119934,
      "learning_rate": 1.7752509435477397e-05,
      "loss": 0.0219,
      "step": 42000
    },
    {
      "epoch": 0.5688053748762012,
      "grad_norm": 0.013100198470056057,
      "learning_rate": 1.7725742123718517e-05,
      "loss": 0.045,
      "step": 42500
    },
    {
      "epoch": 0.5754972028159212,
      "grad_norm": 0.009543226100504398,
      "learning_rate": 1.7698974811959637e-05,
      "loss": 0.0338,
      "step": 43000
    },
    {
      "epoch": 0.5821890307556412,
      "grad_norm": 0.154267355799675,
      "learning_rate": 1.7672261034824272e-05,
      "loss": 0.0485,
      "step": 43500
    },
    {
      "epoch": 0.5888808586953612,
      "grad_norm": 0.00789195578545332,
      "learning_rate": 1.764554725768891e-05,
      "loss": 0.0528,
      "step": 44000
    },
    {
      "epoch": 0.5955726866350812,
      "grad_norm": 0.004760543815791607,
      "learning_rate": 1.761877994593003e-05,
      "loss": 0.0335,
      "step": 44500
    },
    {
      "epoch": 0.6022645145748012,
      "grad_norm": 0.006895027589052916,
      "learning_rate": 1.759201263417115e-05,
      "loss": 0.0385,
      "step": 45000
    },
    {
      "epoch": 0.6089563425145212,
      "grad_norm": 29.424293518066406,
      "learning_rate": 1.7565245322412272e-05,
      "loss": 0.0975,
      "step": 45500
    },
    {
      "epoch": 0.6156481704542413,
      "grad_norm": 0.14066633582115173,
      "learning_rate": 1.7538478010653392e-05,
      "loss": 0.0401,
      "step": 46000
    },
    {
      "epoch": 0.6223399983939613,
      "grad_norm": 11.78432559967041,
      "learning_rate": 1.751171069889451e-05,
      "loss": 0.034,
      "step": 46500
    },
    {
      "epoch": 0.6290318263336813,
      "grad_norm": 3.980072021484375,
      "learning_rate": 1.748494338713563e-05,
      "loss": 0.0352,
      "step": 47000
    },
    {
      "epoch": 0.6357236542734013,
      "grad_norm": 0.005017422139644623,
      "learning_rate": 1.7458176075376753e-05,
      "loss": 0.0367,
      "step": 47500
    },
    {
      "epoch": 0.6424154822131214,
      "grad_norm": 0.018190061673521996,
      "learning_rate": 1.7431408763617873e-05,
      "loss": 0.0455,
      "step": 48000
    },
    {
      "epoch": 0.6491073101528414,
      "grad_norm": 0.0690702423453331,
      "learning_rate": 1.740469498648251e-05,
      "loss": 0.0439,
      "step": 48500
    },
    {
      "epoch": 0.6557991380925614,
      "grad_norm": 0.02622833661735058,
      "learning_rate": 1.737792767472363e-05,
      "loss": 0.0534,
      "step": 49000
    },
    {
      "epoch": 0.6624909660322814,
      "grad_norm": 0.007410981692373753,
      "learning_rate": 1.7351213897588268e-05,
      "loss": 0.0565,
      "step": 49500
    },
    {
      "epoch": 0.6691827939720014,
      "grad_norm": 0.023615024983882904,
      "learning_rate": 1.7324446585829388e-05,
      "loss": 0.0303,
      "step": 50000
    },
    {
      "epoch": 0.6758746219117214,
      "grad_norm": 25.32433319091797,
      "learning_rate": 1.7297732808694023e-05,
      "loss": 0.0354,
      "step": 50500
    },
    {
      "epoch": 0.6825664498514414,
      "grad_norm": 0.016241949051618576,
      "learning_rate": 1.7270965496935144e-05,
      "loss": 0.0522,
      "step": 51000
    },
    {
      "epoch": 0.6892582777911614,
      "grad_norm": 0.02320280112326145,
      "learning_rate": 1.7244198185176264e-05,
      "loss": 0.0315,
      "step": 51500
    },
    {
      "epoch": 0.6959501057308815,
      "grad_norm": 0.14172767102718353,
      "learning_rate": 1.7217430873417384e-05,
      "loss": 0.0361,
      "step": 52000
    },
    {
      "epoch": 0.7026419336706015,
      "grad_norm": 0.0428684763610363,
      "learning_rate": 1.7190663561658504e-05,
      "loss": 0.0504,
      "step": 52500
    },
    {
      "epoch": 0.7093337616103215,
      "grad_norm": 0.006326210219413042,
      "learning_rate": 1.7163896249899625e-05,
      "loss": 0.0396,
      "step": 53000
    },
    {
      "epoch": 0.7160255895500415,
      "grad_norm": 0.006577476859092712,
      "learning_rate": 1.7137128938140745e-05,
      "loss": 0.0414,
      "step": 53500
    },
    {
      "epoch": 0.7227174174897615,
      "grad_norm": 0.005494531709700823,
      "learning_rate": 1.7110415161005384e-05,
      "loss": 0.0397,
      "step": 54000
    },
    {
      "epoch": 0.7294092454294815,
      "grad_norm": 0.051431700587272644,
      "learning_rate": 1.708370138387002e-05,
      "loss": 0.0369,
      "step": 54500
    },
    {
      "epoch": 0.7361010733692015,
      "grad_norm": 0.03829536586999893,
      "learning_rate": 1.705693407211114e-05,
      "loss": 0.0581,
      "step": 55000
    },
    {
      "epoch": 0.7427929013089215,
      "grad_norm": 0.015044815838336945,
      "learning_rate": 1.703016676035226e-05,
      "loss": 0.0415,
      "step": 55500
    },
    {
      "epoch": 0.7494847292486415,
      "grad_norm": 0.024291738867759705,
      "learning_rate": 1.700339944859338e-05,
      "loss": 0.0393,
      "step": 56000
    },
    {
      "epoch": 0.7561765571883615,
      "grad_norm": 0.008540622889995575,
      "learning_rate": 1.69766321368345e-05,
      "loss": 0.0343,
      "step": 56500
    },
    {
      "epoch": 0.7628683851280816,
      "grad_norm": 0.040114279836416245,
      "learning_rate": 1.694986482507562e-05,
      "loss": 0.0434,
      "step": 57000
    },
    {
      "epoch": 0.7695602130678016,
      "grad_norm": 0.015358100645244122,
      "learning_rate": 1.692309751331674e-05,
      "loss": 0.0392,
      "step": 57500
    },
    {
      "epoch": 0.7762520410075217,
      "grad_norm": 4.507519245147705,
      "learning_rate": 1.6896330201557858e-05,
      "loss": 0.0288,
      "step": 58000
    },
    {
      "epoch": 0.7829438689472417,
      "grad_norm": 0.007302429527044296,
      "learning_rate": 1.6869562889798978e-05,
      "loss": 0.037,
      "step": 58500
    },
    {
      "epoch": 0.7896356968869617,
      "grad_norm": 0.027282489463686943,
      "learning_rate": 1.6842795578040098e-05,
      "loss": 0.0505,
      "step": 59000
    },
    {
      "epoch": 0.7963275248266817,
      "grad_norm": 0.0435088686645031,
      "learning_rate": 1.681602826628122e-05,
      "loss": 0.0372,
      "step": 59500
    },
    {
      "epoch": 0.8030193527664017,
      "grad_norm": 0.004382804036140442,
      "learning_rate": 1.678926095452234e-05,
      "loss": 0.041,
      "step": 60000
    },
    {
      "epoch": 0.8097111807061217,
      "grad_norm": 0.0889260470867157,
      "learning_rate": 1.676249364276346e-05,
      "loss": 0.0392,
      "step": 60500
    },
    {
      "epoch": 0.8164030086458417,
      "grad_norm": 0.6129966974258423,
      "learning_rate": 1.673572633100458e-05,
      "loss": 0.0457,
      "step": 61000
    },
    {
      "epoch": 0.8230948365855617,
      "grad_norm": 0.12050234526395798,
      "learning_rate": 1.67089590192457e-05,
      "loss": 0.0393,
      "step": 61500
    },
    {
      "epoch": 0.8297866645252817,
      "grad_norm": 0.023022426292300224,
      "learning_rate": 1.6682245242110335e-05,
      "loss": 0.0381,
      "step": 62000
    },
    {
      "epoch": 0.8364784924650017,
      "grad_norm": 0.12084700167179108,
      "learning_rate": 1.6655477930351455e-05,
      "loss": 0.04,
      "step": 62500
    },
    {
      "epoch": 0.8431703204047217,
      "grad_norm": 0.06323599070310593,
      "learning_rate": 1.6628764153216094e-05,
      "loss": 0.0464,
      "step": 63000
    },
    {
      "epoch": 0.8498621483444417,
      "grad_norm": 0.056267641484737396,
      "learning_rate": 1.6601996841457214e-05,
      "loss": 0.0309,
      "step": 63500
    },
    {
      "epoch": 0.8565539762841617,
      "grad_norm": 0.13685142993927002,
      "learning_rate": 1.6575229529698335e-05,
      "loss": 0.0382,
      "step": 64000
    },
    {
      "epoch": 0.8632458042238818,
      "grad_norm": 0.01324182003736496,
      "learning_rate": 1.6548462217939455e-05,
      "loss": 0.048,
      "step": 64500
    },
    {
      "epoch": 0.8699376321636018,
      "grad_norm": 25.4251766204834,
      "learning_rate": 1.652174844080409e-05,
      "loss": 0.0622,
      "step": 65000
    },
    {
      "epoch": 0.8766294601033219,
      "grad_norm": 0.019407005980610847,
      "learning_rate": 1.649498112904521e-05,
      "loss": 0.034,
      "step": 65500
    },
    {
      "epoch": 0.8833212880430419,
      "grad_norm": 0.013158817775547504,
      "learning_rate": 1.646821381728633e-05,
      "loss": 0.04,
      "step": 66000
    },
    {
      "epoch": 0.8900131159827619,
      "grad_norm": 0.022213362157344818,
      "learning_rate": 1.644144650552745e-05,
      "loss": 0.0424,
      "step": 66500
    },
    {
      "epoch": 0.8967049439224819,
      "grad_norm": 0.016293920576572418,
      "learning_rate": 1.641467919376857e-05,
      "loss": 0.0441,
      "step": 67000
    },
    {
      "epoch": 0.9033967718622019,
      "grad_norm": 0.11932259798049927,
      "learning_rate": 1.6387911882009692e-05,
      "loss": 0.0444,
      "step": 67500
    },
    {
      "epoch": 0.9100885998019219,
      "grad_norm": 0.03209853172302246,
      "learning_rate": 1.636119810487433e-05,
      "loss": 0.0358,
      "step": 68000
    },
    {
      "epoch": 0.9167804277416419,
      "grad_norm": 0.03486077859997749,
      "learning_rate": 1.6334430793115447e-05,
      "loss": 0.0389,
      "step": 68500
    },
    {
      "epoch": 0.9234722556813619,
      "grad_norm": 0.015240680426359177,
      "learning_rate": 1.6307663481356568e-05,
      "loss": 0.0446,
      "step": 69000
    },
    {
      "epoch": 0.9301640836210819,
      "grad_norm": 0.015254886820912361,
      "learning_rate": 1.6280896169597688e-05,
      "loss": 0.0682,
      "step": 69500
    },
    {
      "epoch": 0.936855911560802,
      "grad_norm": 3.8672003746032715,
      "learning_rate": 1.6254128857838808e-05,
      "loss": 0.0895,
      "step": 70000
    },
    {
      "epoch": 0.943547739500522,
      "grad_norm": 42.00361633300781,
      "learning_rate": 1.622736154607993e-05,
      "loss": 0.0785,
      "step": 70500
    },
    {
      "epoch": 0.950239567440242,
      "grad_norm": 0.017758021131157875,
      "learning_rate": 1.620059423432105e-05,
      "loss": 0.0711,
      "step": 71000
    },
    {
      "epoch": 0.956931395379962,
      "grad_norm": 0.020184917375445366,
      "learning_rate": 1.617382692256217e-05,
      "loss": 0.0543,
      "step": 71500
    },
    {
      "epoch": 0.963623223319682,
      "grad_norm": 0.018102100118994713,
      "learning_rate": 1.614705961080329e-05,
      "loss": 0.0491,
      "step": 72000
    },
    {
      "epoch": 0.970315051259402,
      "grad_norm": 0.02041660249233246,
      "learning_rate": 1.612029229904441e-05,
      "loss": 0.0601,
      "step": 72500
    },
    {
      "epoch": 0.977006879199122,
      "grad_norm": 0.016586367040872574,
      "learning_rate": 1.609352498728553e-05,
      "loss": 0.0468,
      "step": 73000
    },
    {
      "epoch": 0.983698707138842,
      "grad_norm": 0.010597994551062584,
      "learning_rate": 1.6066757675526647e-05,
      "loss": 0.0299,
      "step": 73500
    },
    {
      "epoch": 0.990390535078562,
      "grad_norm": 0.033980842679739,
      "learning_rate": 1.6039990363767767e-05,
      "loss": 0.0607,
      "step": 74000
    },
    {
      "epoch": 0.9970823630182821,
      "grad_norm": 0.016405552625656128,
      "learning_rate": 1.6013276586632406e-05,
      "loss": 0.0406,
      "step": 74500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9840065312240691,
      "eval_loss": 0.07214654982089996,
      "eval_runtime": 143.2855,
      "eval_samples_per_second": 521.462,
      "eval_steps_per_second": 65.185,
      "step": 74718
    },
    {
      "epoch": 1.003774190958002,
      "grad_norm": 0.02102821134030819,
      "learning_rate": 1.5986509274873526e-05,
      "loss": 0.0569,
      "step": 75000
    },
    {
      "epoch": 1.0104660188977221,
      "grad_norm": 0.024627545848488808,
      "learning_rate": 1.5959741963114646e-05,
      "loss": 0.0712,
      "step": 75500
    },
    {
      "epoch": 1.017157846837442,
      "grad_norm": 11.847190856933594,
      "learning_rate": 1.5932974651355767e-05,
      "loss": 0.0505,
      "step": 76000
    },
    {
      "epoch": 1.0238496747771622,
      "grad_norm": 0.017919247969985008,
      "learning_rate": 1.5906260874220402e-05,
      "loss": 0.042,
      "step": 76500
    },
    {
      "epoch": 1.030541502716882,
      "grad_norm": 0.01854877360165119,
      "learning_rate": 1.5879493562461522e-05,
      "loss": 0.0433,
      "step": 77000
    },
    {
      "epoch": 1.0372333306566022,
      "grad_norm": 0.01307060569524765,
      "learning_rate": 1.5852726250702643e-05,
      "loss": 0.0447,
      "step": 77500
    },
    {
      "epoch": 1.043925158596322,
      "grad_norm": 89.74649047851562,
      "learning_rate": 1.5825958938943763e-05,
      "loss": 0.0456,
      "step": 78000
    },
    {
      "epoch": 1.0506169865360422,
      "grad_norm": 0.01767083816230297,
      "learning_rate": 1.5799191627184883e-05,
      "loss": 0.0354,
      "step": 78500
    },
    {
      "epoch": 1.0573088144757623,
      "grad_norm": 26.644878387451172,
      "learning_rate": 1.5772477850049522e-05,
      "loss": 0.0559,
      "step": 79000
    },
    {
      "epoch": 1.0640006424154822,
      "grad_norm": 0.0608225092291832,
      "learning_rate": 1.5745710538290642e-05,
      "loss": 0.0326,
      "step": 79500
    },
    {
      "epoch": 1.0706924703552023,
      "grad_norm": 0.020465107634663582,
      "learning_rate": 1.571894322653176e-05,
      "loss": 0.0504,
      "step": 80000
    },
    {
      "epoch": 1.0773842982949222,
      "grad_norm": 0.023333575576543808,
      "learning_rate": 1.569217591477288e-05,
      "loss": 0.0331,
      "step": 80500
    },
    {
      "epoch": 1.0840761262346423,
      "grad_norm": 0.24663817882537842,
      "learning_rate": 1.5665408603014e-05,
      "loss": 0.0383,
      "step": 81000
    },
    {
      "epoch": 1.0907679541743622,
      "grad_norm": 0.005013224668800831,
      "learning_rate": 1.5638748360502157e-05,
      "loss": 0.0435,
      "step": 81500
    },
    {
      "epoch": 1.0974597821140823,
      "grad_norm": 0.013139571994543076,
      "learning_rate": 1.5612034583366792e-05,
      "loss": 0.0486,
      "step": 82000
    },
    {
      "epoch": 1.1041516100538022,
      "grad_norm": 0.01658661849796772,
      "learning_rate": 1.5585267271607912e-05,
      "loss": 0.0254,
      "step": 82500
    },
    {
      "epoch": 1.1108434379935224,
      "grad_norm": 0.09269557148218155,
      "learning_rate": 1.5558499959849033e-05,
      "loss": 0.0376,
      "step": 83000
    },
    {
      "epoch": 1.1175352659332423,
      "grad_norm": 0.19779546558856964,
      "learning_rate": 1.5531732648090153e-05,
      "loss": 0.1053,
      "step": 83500
    },
    {
      "epoch": 1.1242270938729624,
      "grad_norm": 0.030784372240304947,
      "learning_rate": 1.5504965336331273e-05,
      "loss": 0.0508,
      "step": 84000
    },
    {
      "epoch": 1.1309189218126823,
      "grad_norm": 0.06557115167379379,
      "learning_rate": 1.5478198024572393e-05,
      "loss": 0.0476,
      "step": 84500
    },
    {
      "epoch": 1.1376107497524024,
      "grad_norm": 0.01329509261995554,
      "learning_rate": 1.5451430712813514e-05,
      "loss": 0.0363,
      "step": 85000
    },
    {
      "epoch": 1.1443025776921223,
      "grad_norm": 0.009438504464924335,
      "learning_rate": 1.5424663401054634e-05,
      "loss": 0.0266,
      "step": 85500
    },
    {
      "epoch": 1.1509944056318424,
      "grad_norm": 0.04822155833244324,
      "learning_rate": 1.5397896089295754e-05,
      "loss": 0.0376,
      "step": 86000
    },
    {
      "epoch": 1.1576862335715625,
      "grad_norm": 0.04102310910820961,
      "learning_rate": 1.537118231216039e-05,
      "loss": 0.0405,
      "step": 86500
    },
    {
      "epoch": 1.1643780615112824,
      "grad_norm": 0.039247360080480576,
      "learning_rate": 1.534441500040151e-05,
      "loss": 0.0667,
      "step": 87000
    },
    {
      "epoch": 1.1710698894510023,
      "grad_norm": 0.038806553930044174,
      "learning_rate": 1.531764768864263e-05,
      "loss": 0.0377,
      "step": 87500
    },
    {
      "epoch": 1.1777617173907224,
      "grad_norm": 0.030093537643551826,
      "learning_rate": 1.529088037688375e-05,
      "loss": 0.0366,
      "step": 88000
    },
    {
      "epoch": 1.1844535453304426,
      "grad_norm": 0.0578249990940094,
      "learning_rate": 1.526411306512487e-05,
      "loss": 0.0461,
      "step": 88500
    },
    {
      "epoch": 1.1911453732701625,
      "grad_norm": 0.010320750065147877,
      "learning_rate": 1.5237345753365991e-05,
      "loss": 0.0296,
      "step": 89000
    },
    {
      "epoch": 1.1978372012098826,
      "grad_norm": 0.043146319687366486,
      "learning_rate": 1.5210631976230628e-05,
      "loss": 0.0391,
      "step": 89500
    },
    {
      "epoch": 1.2045290291496025,
      "grad_norm": 0.02805778756737709,
      "learning_rate": 1.5183864664471748e-05,
      "loss": 0.0588,
      "step": 90000
    },
    {
      "epoch": 1.2112208570893226,
      "grad_norm": 0.01797872595489025,
      "learning_rate": 1.5157097352712869e-05,
      "loss": 0.0318,
      "step": 90500
    },
    {
      "epoch": 1.2179126850290425,
      "grad_norm": 0.06816426664590836,
      "learning_rate": 1.5130330040953989e-05,
      "loss": 0.0417,
      "step": 91000
    },
    {
      "epoch": 1.2246045129687626,
      "grad_norm": 0.021450074389576912,
      "learning_rate": 1.510356272919511e-05,
      "loss": 0.0456,
      "step": 91500
    },
    {
      "epoch": 1.2312963409084825,
      "grad_norm": 0.007763643283396959,
      "learning_rate": 1.5076795417436228e-05,
      "loss": 0.0362,
      "step": 92000
    },
    {
      "epoch": 1.2379881688482026,
      "grad_norm": 0.01165120117366314,
      "learning_rate": 1.5050028105677348e-05,
      "loss": 0.0392,
      "step": 92500
    },
    {
      "epoch": 1.2446799967879225,
      "grad_norm": 0.013199067674577236,
      "learning_rate": 1.5023260793918469e-05,
      "loss": 0.0324,
      "step": 93000
    },
    {
      "epoch": 1.2513718247276426,
      "grad_norm": 0.05698908492922783,
      "learning_rate": 1.4996493482159589e-05,
      "loss": 0.039,
      "step": 93500
    },
    {
      "epoch": 1.2580636526673628,
      "grad_norm": 7.417758464813232,
      "learning_rate": 1.4969726170400707e-05,
      "loss": 0.0382,
      "step": 94000
    },
    {
      "epoch": 1.2647554806070826,
      "grad_norm": 0.014565900899469852,
      "learning_rate": 1.4942958858641828e-05,
      "loss": 0.0392,
      "step": 94500
    },
    {
      "epoch": 1.2714473085468025,
      "grad_norm": 0.0052986349910497665,
      "learning_rate": 1.4916245081506466e-05,
      "loss": 0.0228,
      "step": 95000
    },
    {
      "epoch": 1.2781391364865227,
      "grad_norm": 0.03638508543372154,
      "learning_rate": 1.4889477769747585e-05,
      "loss": 0.0386,
      "step": 95500
    },
    {
      "epoch": 1.2848309644262428,
      "grad_norm": 0.00800937321037054,
      "learning_rate": 1.4862710457988705e-05,
      "loss": 0.0311,
      "step": 96000
    },
    {
      "epoch": 1.2915227923659627,
      "grad_norm": 0.013006594032049179,
      "learning_rate": 1.4835943146229826e-05,
      "loss": 0.0364,
      "step": 96500
    },
    {
      "epoch": 1.2982146203056826,
      "grad_norm": 0.02121400274336338,
      "learning_rate": 1.4809175834470946e-05,
      "loss": 0.0381,
      "step": 97000
    },
    {
      "epoch": 1.3049064482454027,
      "grad_norm": 0.017525535076856613,
      "learning_rate": 1.4782408522712066e-05,
      "loss": 0.0429,
      "step": 97500
    },
    {
      "epoch": 1.3115982761851228,
      "grad_norm": 0.019446944817900658,
      "learning_rate": 1.4755694745576703e-05,
      "loss": 0.0337,
      "step": 98000
    },
    {
      "epoch": 1.3182901041248427,
      "grad_norm": 3.649751901626587,
      "learning_rate": 1.4728927433817823e-05,
      "loss": 0.0344,
      "step": 98500
    },
    {
      "epoch": 1.3249819320645628,
      "grad_norm": 5.589725971221924,
      "learning_rate": 1.4702160122058944e-05,
      "loss": 0.03,
      "step": 99000
    },
    {
      "epoch": 1.3316737600042827,
      "grad_norm": 0.017418405041098595,
      "learning_rate": 1.4675392810300062e-05,
      "loss": 0.0341,
      "step": 99500
    },
    {
      "epoch": 1.3383655879440028,
      "grad_norm": 0.029940037056803703,
      "learning_rate": 1.4648679033164701e-05,
      "loss": 0.0493,
      "step": 100000
    },
    {
      "epoch": 1.3450574158837227,
      "grad_norm": 0.22235819697380066,
      "learning_rate": 1.4621911721405821e-05,
      "loss": 0.059,
      "step": 100500
    },
    {
      "epoch": 1.3517492438234429,
      "grad_norm": 0.07213294506072998,
      "learning_rate": 1.459514440964694e-05,
      "loss": 0.0427,
      "step": 101000
    },
    {
      "epoch": 1.3584410717631628,
      "grad_norm": 0.017565639689564705,
      "learning_rate": 1.456837709788806e-05,
      "loss": 0.0279,
      "step": 101500
    },
    {
      "epoch": 1.3651328997028829,
      "grad_norm": 0.016205454245209694,
      "learning_rate": 1.454160978612918e-05,
      "loss": 0.0447,
      "step": 102000
    },
    {
      "epoch": 1.3718247276426028,
      "grad_norm": 0.02381316013634205,
      "learning_rate": 1.4514896008993817e-05,
      "loss": 0.0449,
      "step": 102500
    },
    {
      "epoch": 1.3785165555823229,
      "grad_norm": 0.03551754727959633,
      "learning_rate": 1.4488128697234938e-05,
      "loss": 0.035,
      "step": 103000
    },
    {
      "epoch": 1.385208383522043,
      "grad_norm": 0.12012482434511185,
      "learning_rate": 1.4461361385476058e-05,
      "loss": 0.055,
      "step": 103500
    },
    {
      "epoch": 1.391900211461763,
      "grad_norm": 0.02220664918422699,
      "learning_rate": 1.4434594073717178e-05,
      "loss": 0.0403,
      "step": 104000
    },
    {
      "epoch": 1.3985920394014828,
      "grad_norm": 0.03510086610913277,
      "learning_rate": 1.4407826761958297e-05,
      "loss": 0.0644,
      "step": 104500
    },
    {
      "epoch": 1.405283867341203,
      "grad_norm": 0.018089327961206436,
      "learning_rate": 1.4381059450199417e-05,
      "loss": 0.0422,
      "step": 105000
    },
    {
      "epoch": 1.411975695280923,
      "grad_norm": 0.01667984575033188,
      "learning_rate": 1.4354292138440538e-05,
      "loss": 0.0463,
      "step": 105500
    },
    {
      "epoch": 1.418667523220643,
      "grad_norm": 0.011023368686437607,
      "learning_rate": 1.4327578361305175e-05,
      "loss": 0.0547,
      "step": 106000
    },
    {
      "epoch": 1.4253593511603628,
      "grad_norm": 0.017395727336406708,
      "learning_rate": 1.4300864584169813e-05,
      "loss": 0.0415,
      "step": 106500
    },
    {
      "epoch": 1.432051179100083,
      "grad_norm": 8.840470314025879,
      "learning_rate": 1.4274097272410933e-05,
      "loss": 0.0336,
      "step": 107000
    },
    {
      "epoch": 1.438743007039803,
      "grad_norm": 0.03199724480509758,
      "learning_rate": 1.4247329960652052e-05,
      "loss": 0.0453,
      "step": 107500
    },
    {
      "epoch": 1.445434834979523,
      "grad_norm": 0.04399611055850983,
      "learning_rate": 1.4220562648893172e-05,
      "loss": 0.0585,
      "step": 108000
    },
    {
      "epoch": 1.452126662919243,
      "grad_norm": 0.0176805779337883,
      "learning_rate": 1.4193795337134293e-05,
      "loss": 0.0331,
      "step": 108500
    },
    {
      "epoch": 1.458818490858963,
      "grad_norm": 3.731849193572998,
      "learning_rate": 1.4167028025375413e-05,
      "loss": 0.0591,
      "step": 109000
    },
    {
      "epoch": 1.465510318798683,
      "grad_norm": 0.022814054042100906,
      "learning_rate": 1.4140260713616533e-05,
      "loss": 0.0452,
      "step": 109500
    },
    {
      "epoch": 1.472202146738403,
      "grad_norm": 7.446138381958008,
      "learning_rate": 1.411354693648117e-05,
      "loss": 0.0564,
      "step": 110000
    },
    {
      "epoch": 1.4788939746781231,
      "grad_norm": 0.025479143485426903,
      "learning_rate": 1.408677962472229e-05,
      "loss": 0.043,
      "step": 110500
    },
    {
      "epoch": 1.485585802617843,
      "grad_norm": 0.056819166988134384,
      "learning_rate": 1.406001231296341e-05,
      "loss": 0.039,
      "step": 111000
    },
    {
      "epoch": 1.4922776305575631,
      "grad_norm": 3.6375038623809814,
      "learning_rate": 1.403324500120453e-05,
      "loss": 0.0545,
      "step": 111500
    },
    {
      "epoch": 1.498969458497283,
      "grad_norm": 0.06564651429653168,
      "learning_rate": 1.400647768944565e-05,
      "loss": 0.0282,
      "step": 112000
    },
    {
      "epoch": 1.5056612864370031,
      "grad_norm": 0.06722092628479004,
      "learning_rate": 1.397971037768677e-05,
      "loss": 0.0472,
      "step": 112500
    },
    {
      "epoch": 1.5123531143767233,
      "grad_norm": 0.07936404645442963,
      "learning_rate": 1.395294306592789e-05,
      "loss": 0.085,
      "step": 113000
    },
    {
      "epoch": 1.5190449423164432,
      "grad_norm": 0.037150606513023376,
      "learning_rate": 1.3926175754169009e-05,
      "loss": 0.0463,
      "step": 113500
    },
    {
      "epoch": 1.525736770256163,
      "grad_norm": 0.03264281898736954,
      "learning_rate": 1.389940844241013e-05,
      "loss": 0.0331,
      "step": 114000
    },
    {
      "epoch": 1.5324285981958832,
      "grad_norm": 0.09330461919307709,
      "learning_rate": 1.387264113065125e-05,
      "loss": 0.0452,
      "step": 114500
    },
    {
      "epoch": 1.5391204261356033,
      "grad_norm": 0.050791043788194656,
      "learning_rate": 1.384587381889237e-05,
      "loss": 0.0744,
      "step": 115000
    },
    {
      "epoch": 1.5458122540753232,
      "grad_norm": 0.028209231793880463,
      "learning_rate": 1.381910650713349e-05,
      "loss": 0.0398,
      "step": 115500
    },
    {
      "epoch": 1.552504082015043,
      "grad_norm": 0.0324074812233448,
      "learning_rate": 1.3792339195374609e-05,
      "loss": 0.0481,
      "step": 116000
    },
    {
      "epoch": 1.5591959099547632,
      "grad_norm": 3.863939046859741,
      "learning_rate": 1.3765625418239247e-05,
      "loss": 0.0554,
      "step": 116500
    },
    {
      "epoch": 1.5658877378944833,
      "grad_norm": 0.05868634581565857,
      "learning_rate": 1.3738858106480368e-05,
      "loss": 0.0633,
      "step": 117000
    },
    {
      "epoch": 1.5725795658342032,
      "grad_norm": 0.037994105368852615,
      "learning_rate": 1.3712090794721486e-05,
      "loss": 0.0414,
      "step": 117500
    },
    {
      "epoch": 1.5792713937739233,
      "grad_norm": 4.7385382652282715,
      "learning_rate": 1.3685323482962607e-05,
      "loss": 0.0468,
      "step": 118000
    },
    {
      "epoch": 1.5859632217136435,
      "grad_norm": 0.03836880624294281,
      "learning_rate": 1.3658556171203727e-05,
      "loss": 0.0427,
      "step": 118500
    },
    {
      "epoch": 1.5926550496533634,
      "grad_norm": 0.01194749865680933,
      "learning_rate": 1.3631788859444847e-05,
      "loss": 0.0209,
      "step": 119000
    },
    {
      "epoch": 1.5993468775930832,
      "grad_norm": 0.018777653574943542,
      "learning_rate": 1.3605021547685966e-05,
      "loss": 0.0439,
      "step": 119500
    },
    {
      "epoch": 1.6060387055328034,
      "grad_norm": 0.04356293007731438,
      "learning_rate": 1.3578254235927086e-05,
      "loss": 0.035,
      "step": 120000
    },
    {
      "epoch": 1.6127305334725235,
      "grad_norm": 0.004735275637358427,
      "learning_rate": 1.3551486924168207e-05,
      "loss": 0.0243,
      "step": 120500
    },
    {
      "epoch": 1.6194223614122434,
      "grad_norm": 0.015763241797685623,
      "learning_rate": 1.3524719612409327e-05,
      "loss": 0.0343,
      "step": 121000
    },
    {
      "epoch": 1.6261141893519633,
      "grad_norm": 0.020778896287083626,
      "learning_rate": 1.3497952300650447e-05,
      "loss": 0.0386,
      "step": 121500
    },
    {
      "epoch": 1.6328060172916834,
      "grad_norm": 0.028657691553235054,
      "learning_rate": 1.3471184988891566e-05,
      "loss": 0.0457,
      "step": 122000
    },
    {
      "epoch": 1.6394978452314035,
      "grad_norm": 0.0075733596459031105,
      "learning_rate": 1.3444417677132686e-05,
      "loss": 0.0195,
      "step": 122500
    },
    {
      "epoch": 1.6461896731711234,
      "grad_norm": 0.02366497553884983,
      "learning_rate": 1.3417650365373806e-05,
      "loss": 0.0384,
      "step": 123000
    },
    {
      "epoch": 1.6528815011108433,
      "grad_norm": 0.1062995195388794,
      "learning_rate": 1.3390883053614927e-05,
      "loss": 0.0357,
      "step": 123500
    },
    {
      "epoch": 1.6595733290505634,
      "grad_norm": 0.1449146270751953,
      "learning_rate": 1.3364115741856047e-05,
      "loss": 0.0454,
      "step": 124000
    },
    {
      "epoch": 1.6662651569902835,
      "grad_norm": 0.01711772382259369,
      "learning_rate": 1.3337348430097166e-05,
      "loss": 0.0308,
      "step": 124500
    },
    {
      "epoch": 1.6729569849300034,
      "grad_norm": 0.015910351648926735,
      "learning_rate": 1.3310688187585321e-05,
      "loss": 0.0355,
      "step": 125000
    },
    {
      "epoch": 1.6796488128697233,
      "grad_norm": 0.020999716594815254,
      "learning_rate": 1.3283920875826441e-05,
      "loss": 0.0345,
      "step": 125500
    },
    {
      "epoch": 1.6863406408094435,
      "grad_norm": 0.05663260444998741,
      "learning_rate": 1.3257153564067561e-05,
      "loss": 0.0488,
      "step": 126000
    },
    {
      "epoch": 1.6930324687491636,
      "grad_norm": 0.017926068976521492,
      "learning_rate": 1.3230386252308682e-05,
      "loss": 0.0437,
      "step": 126500
    },
    {
      "epoch": 1.6997242966888835,
      "grad_norm": 0.03161977604031563,
      "learning_rate": 1.3203618940549802e-05,
      "loss": 0.0514,
      "step": 127000
    },
    {
      "epoch": 1.7064161246286036,
      "grad_norm": 0.026914365589618683,
      "learning_rate": 1.317685162879092e-05,
      "loss": 0.0397,
      "step": 127500
    },
    {
      "epoch": 1.7131079525683237,
      "grad_norm": 0.1037825420498848,
      "learning_rate": 1.3150084317032041e-05,
      "loss": 0.059,
      "step": 128000
    },
    {
      "epoch": 1.7197997805080436,
      "grad_norm": 0.04650367796421051,
      "learning_rate": 1.3123317005273161e-05,
      "loss": 0.0548,
      "step": 128500
    },
    {
      "epoch": 1.7264916084477635,
      "grad_norm": 0.03472224622964859,
      "learning_rate": 1.3096656762761317e-05,
      "loss": 0.0632,
      "step": 129000
    },
    {
      "epoch": 1.7331834363874836,
      "grad_norm": 0.0226438008248806,
      "learning_rate": 1.3069889451002437e-05,
      "loss": 0.0558,
      "step": 129500
    },
    {
      "epoch": 1.7398752643272037,
      "grad_norm": 0.026432180777192116,
      "learning_rate": 1.3043175673867074e-05,
      "loss": 0.0362,
      "step": 130000
    },
    {
      "epoch": 1.7465670922669236,
      "grad_norm": 0.32273781299591064,
      "learning_rate": 1.3016408362108194e-05,
      "loss": 0.1052,
      "step": 130500
    },
    {
      "epoch": 1.7532589202066435,
      "grad_norm": 0.15867729485034943,
      "learning_rate": 1.2989641050349316e-05,
      "loss": 0.1091,
      "step": 131000
    },
    {
      "epoch": 1.7599507481463637,
      "grad_norm": 0.08895464986562729,
      "learning_rate": 1.2962927273213951e-05,
      "loss": 0.0667,
      "step": 131500
    },
    {
      "epoch": 1.7666425760860838,
      "grad_norm": 0.05708571523427963,
      "learning_rate": 1.2936213496078588e-05,
      "loss": 0.0609,
      "step": 132000
    },
    {
      "epoch": 1.7733344040258037,
      "grad_norm": 0.034740228205919266,
      "learning_rate": 1.2909446184319709e-05,
      "loss": 0.0582,
      "step": 132500
    },
    {
      "epoch": 1.7800262319655236,
      "grad_norm": 0.14775212109088898,
      "learning_rate": 1.2882785941807866e-05,
      "loss": 0.0851,
      "step": 133000
    },
    {
      "epoch": 1.7867180599052437,
      "grad_norm": 0.03092982806265354,
      "learning_rate": 1.2856072164672504e-05,
      "loss": 0.0518,
      "step": 133500
    },
    {
      "epoch": 1.7934098878449638,
      "grad_norm": 0.03586046025156975,
      "learning_rate": 1.282941192216066e-05,
      "loss": 0.0668,
      "step": 134000
    },
    {
      "epoch": 1.8001017157846837,
      "grad_norm": 0.09080594778060913,
      "learning_rate": 1.2802644610401778e-05,
      "loss": 0.1488,
      "step": 134500
    },
    {
      "epoch": 1.8067935437244036,
      "grad_norm": 0.043096620589494705,
      "learning_rate": 1.2775877298642898e-05,
      "loss": 0.0901,
      "step": 135000
    },
    {
      "epoch": 1.813485371664124,
      "grad_norm": 0.05278338864445686,
      "learning_rate": 1.2749109986884019e-05,
      "loss": 0.0905,
      "step": 135500
    },
    {
      "epoch": 1.8201771996038438,
      "grad_norm": 2.250192165374756,
      "learning_rate": 1.2722342675125139e-05,
      "loss": 0.0847,
      "step": 136000
    },
    {
      "epoch": 1.8268690275435637,
      "grad_norm": 0.09124220162630081,
      "learning_rate": 1.2695575363366258e-05,
      "loss": 0.0737,
      "step": 136500
    },
    {
      "epoch": 1.8335608554832838,
      "grad_norm": 0.09324472397565842,
      "learning_rate": 1.2668808051607378e-05,
      "loss": 0.0524,
      "step": 137000
    },
    {
      "epoch": 1.840252683423004,
      "grad_norm": 0.034724656492471695,
      "learning_rate": 1.2642040739848498e-05,
      "loss": 0.0588,
      "step": 137500
    },
    {
      "epoch": 1.8469445113627239,
      "grad_norm": 0.02755643054842949,
      "learning_rate": 1.2615273428089619e-05,
      "loss": 0.0476,
      "step": 138000
    },
    {
      "epoch": 1.8536363393024438,
      "grad_norm": 0.041722849011421204,
      "learning_rate": 1.2588506116330739e-05,
      "loss": 0.0614,
      "step": 138500
    },
    {
      "epoch": 1.8603281672421639,
      "grad_norm": 0.031051188707351685,
      "learning_rate": 1.2561738804571857e-05,
      "loss": 0.06,
      "step": 139000
    },
    {
      "epoch": 1.867019995181884,
      "grad_norm": 4.243847370147705,
      "learning_rate": 1.2534971492812978e-05,
      "loss": 0.0442,
      "step": 139500
    },
    {
      "epoch": 1.873711823121604,
      "grad_norm": 0.0689779743552208,
      "learning_rate": 1.2508204181054098e-05,
      "loss": 0.0538,
      "step": 140000
    },
    {
      "epoch": 1.8804036510613238,
      "grad_norm": 136.09742736816406,
      "learning_rate": 1.2481436869295218e-05,
      "loss": 0.0508,
      "step": 140500
    },
    {
      "epoch": 1.887095479001044,
      "grad_norm": 0.055934514850378036,
      "learning_rate": 1.2454669557536339e-05,
      "loss": 0.0629,
      "step": 141000
    },
    {
      "epoch": 1.893787306940764,
      "grad_norm": 0.07401353120803833,
      "learning_rate": 1.2427902245777457e-05,
      "loss": 0.036,
      "step": 141500
    },
    {
      "epoch": 1.900479134880484,
      "grad_norm": 0.031206905841827393,
      "learning_rate": 1.2401134934018578e-05,
      "loss": 0.0592,
      "step": 142000
    },
    {
      "epoch": 1.9071709628202038,
      "grad_norm": 0.02629106119275093,
      "learning_rate": 1.2374367622259698e-05,
      "loss": 0.0441,
      "step": 142500
    },
    {
      "epoch": 1.913862790759924,
      "grad_norm": 2.8135976791381836,
      "learning_rate": 1.2347600310500818e-05,
      "loss": 0.0619,
      "step": 143000
    },
    {
      "epoch": 1.920554618699644,
      "grad_norm": 0.04473743587732315,
      "learning_rate": 1.2320832998741939e-05,
      "loss": 0.0428,
      "step": 143500
    },
    {
      "epoch": 1.927246446639364,
      "grad_norm": 0.013387288898229599,
      "learning_rate": 1.2294065686983057e-05,
      "loss": 0.0297,
      "step": 144000
    },
    {
      "epoch": 1.933938274579084,
      "grad_norm": 0.04457905516028404,
      "learning_rate": 1.2267298375224177e-05,
      "loss": 0.0385,
      "step": 144500
    },
    {
      "epoch": 1.9406301025188042,
      "grad_norm": 0.08084549754858017,
      "learning_rate": 1.2240531063465298e-05,
      "loss": 0.0376,
      "step": 145000
    },
    {
      "epoch": 1.947321930458524,
      "grad_norm": 0.00888283271342516,
      "learning_rate": 1.2213763751706418e-05,
      "loss": 0.0318,
      "step": 145500
    },
    {
      "epoch": 1.954013758398244,
      "grad_norm": 0.014209375716745853,
      "learning_rate": 1.2186996439947537e-05,
      "loss": 0.0353,
      "step": 146000
    },
    {
      "epoch": 1.960705586337964,
      "grad_norm": 0.01572902500629425,
      "learning_rate": 1.2160229128188657e-05,
      "loss": 0.0522,
      "step": 146500
    },
    {
      "epoch": 1.9673974142776842,
      "grad_norm": 0.012360530905425549,
      "learning_rate": 1.2133461816429777e-05,
      "loss": 0.0398,
      "step": 147000
    },
    {
      "epoch": 1.9740892422174041,
      "grad_norm": 0.05342497304081917,
      "learning_rate": 1.2106694504670898e-05,
      "loss": 0.0488,
      "step": 147500
    },
    {
      "epoch": 1.980781070157124,
      "grad_norm": 0.01695808582007885,
      "learning_rate": 1.2079927192912018e-05,
      "loss": 0.0436,
      "step": 148000
    },
    {
      "epoch": 1.9874728980968441,
      "grad_norm": 0.016566159203648567,
      "learning_rate": 1.2053159881153136e-05,
      "loss": 0.0376,
      "step": 148500
    },
    {
      "epoch": 1.9941647260365643,
      "grad_norm": 0.021183745935559273,
      "learning_rate": 1.2026392569394257e-05,
      "loss": 0.0405,
      "step": 149000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9701410637329693,
      "eval_loss": 0.11011311411857605,
      "eval_runtime": 141.4789,
      "eval_samples_per_second": 528.121,
      "eval_steps_per_second": 66.017,
      "step": 149436
    }
  ],
  "logging_steps": 500,
  "max_steps": 373590,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 1
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5724521012545232e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
