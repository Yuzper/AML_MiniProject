{
  "best_global_step": 74718,
  "best_metric": 0.9840065312240691,
  "best_model_checkpoint": "outputs/checkpoints/bert_full/checkpoint-74718",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 224154,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006691827939720014,
      "grad_norm": 0.04796206206083298,
      "learning_rate": 1.9973286222864637e-05,
      "loss": 0.1402,
      "step": 500
    },
    {
      "epoch": 0.013383655879440027,
      "grad_norm": 0.3119257390499115,
      "learning_rate": 1.9946625980352794e-05,
      "loss": 0.106,
      "step": 1000
    },
    {
      "epoch": 0.020075483819160043,
      "grad_norm": 0.024958932772278786,
      "learning_rate": 1.9919858668593914e-05,
      "loss": 0.1011,
      "step": 1500
    },
    {
      "epoch": 0.026767311758880055,
      "grad_norm": 0.013468916527926922,
      "learning_rate": 1.9893091356835034e-05,
      "loss": 0.0745,
      "step": 2000
    },
    {
      "epoch": 0.03345913969860007,
      "grad_norm": 9.817083358764648,
      "learning_rate": 1.9866377579699673e-05,
      "loss": 0.0799,
      "step": 2500
    },
    {
      "epoch": 0.04015096763832009,
      "grad_norm": 0.03618215397000313,
      "learning_rate": 1.9839663802564312e-05,
      "loss": 0.0617,
      "step": 3000
    },
    {
      "epoch": 0.046842795578040095,
      "grad_norm": 0.005474668461829424,
      "learning_rate": 1.9812896490805432e-05,
      "loss": 0.062,
      "step": 3500
    },
    {
      "epoch": 0.05353462351776011,
      "grad_norm": 0.14690949022769928,
      "learning_rate": 1.978612917904655e-05,
      "loss": 0.0646,
      "step": 4000
    },
    {
      "epoch": 0.060226451457480124,
      "grad_norm": 5.731198310852051,
      "learning_rate": 1.975936186728767e-05,
      "loss": 0.0507,
      "step": 4500
    },
    {
      "epoch": 0.06691827939720014,
      "grad_norm": 0.32261762022972107,
      "learning_rate": 1.973259455552879e-05,
      "loss": 0.0625,
      "step": 5000
    },
    {
      "epoch": 0.07361010733692015,
      "grad_norm": 0.041659314185380936,
      "learning_rate": 1.970582724376991e-05,
      "loss": 0.0549,
      "step": 5500
    },
    {
      "epoch": 0.08030193527664017,
      "grad_norm": 0.0012330882018432021,
      "learning_rate": 1.967905993201103e-05,
      "loss": 0.061,
      "step": 6000
    },
    {
      "epoch": 0.08699376321636018,
      "grad_norm": 0.0013861780753359199,
      "learning_rate": 1.965229262025215e-05,
      "loss": 0.064,
      "step": 6500
    },
    {
      "epoch": 0.09368559115608019,
      "grad_norm": 0.06197873130440712,
      "learning_rate": 1.962557884311679e-05,
      "loss": 0.0539,
      "step": 7000
    },
    {
      "epoch": 0.10037741909580021,
      "grad_norm": 5.5167646408081055,
      "learning_rate": 1.9598811531357906e-05,
      "loss": 0.0516,
      "step": 7500
    },
    {
      "epoch": 0.10706924703552022,
      "grad_norm": 34.947322845458984,
      "learning_rate": 1.9572044219599026e-05,
      "loss": 0.0509,
      "step": 8000
    },
    {
      "epoch": 0.11376107497524024,
      "grad_norm": 0.034620121121406555,
      "learning_rate": 1.9545276907840147e-05,
      "loss": 0.0614,
      "step": 8500
    },
    {
      "epoch": 0.12045290291496025,
      "grad_norm": 0.07794958353042603,
      "learning_rate": 1.9518563130704785e-05,
      "loss": 0.0588,
      "step": 9000
    },
    {
      "epoch": 0.12714473085468025,
      "grad_norm": 0.012425108812749386,
      "learning_rate": 1.9491795818945906e-05,
      "loss": 0.0533,
      "step": 9500
    },
    {
      "epoch": 0.1338365587944003,
      "grad_norm": 0.002988613909110427,
      "learning_rate": 1.9465028507187026e-05,
      "loss": 0.0591,
      "step": 10000
    },
    {
      "epoch": 0.1405283867341203,
      "grad_norm": 0.008023136295378208,
      "learning_rate": 1.9438261195428146e-05,
      "loss": 0.0511,
      "step": 10500
    },
    {
      "epoch": 0.1472202146738403,
      "grad_norm": 0.0007601860561408103,
      "learning_rate": 1.9411493883669266e-05,
      "loss": 0.0384,
      "step": 11000
    },
    {
      "epoch": 0.1539120426135603,
      "grad_norm": 0.04046809300780296,
      "learning_rate": 1.9384780106533902e-05,
      "loss": 0.0456,
      "step": 11500
    },
    {
      "epoch": 0.16060387055328035,
      "grad_norm": 0.1691139191389084,
      "learning_rate": 1.9358012794775022e-05,
      "loss": 0.0511,
      "step": 12000
    },
    {
      "epoch": 0.16729569849300036,
      "grad_norm": 0.09799393266439438,
      "learning_rate": 1.9331245483016142e-05,
      "loss": 0.0525,
      "step": 12500
    },
    {
      "epoch": 0.17398752643272036,
      "grad_norm": 0.10024482756853104,
      "learning_rate": 1.9304478171257263e-05,
      "loss": 0.0584,
      "step": 13000
    },
    {
      "epoch": 0.18067935437244037,
      "grad_norm": 0.004843780770897865,
      "learning_rate": 1.9277710859498383e-05,
      "loss": 0.0531,
      "step": 13500
    },
    {
      "epoch": 0.18737118231216038,
      "grad_norm": 0.0018327045254409313,
      "learning_rate": 1.9250943547739503e-05,
      "loss": 0.0462,
      "step": 14000
    },
    {
      "epoch": 0.1940630102518804,
      "grad_norm": 0.39669570326805115,
      "learning_rate": 1.9224176235980624e-05,
      "loss": 0.0449,
      "step": 14500
    },
    {
      "epoch": 0.20075483819160042,
      "grad_norm": 0.005839720834046602,
      "learning_rate": 1.919746245884526e-05,
      "loss": 0.0407,
      "step": 15000
    },
    {
      "epoch": 0.20744666613132043,
      "grad_norm": 0.008322926238179207,
      "learning_rate": 1.917069514708638e-05,
      "loss": 0.0481,
      "step": 15500
    },
    {
      "epoch": 0.21413849407104044,
      "grad_norm": 58.176979064941406,
      "learning_rate": 1.91439278353275e-05,
      "loss": 0.0372,
      "step": 16000
    },
    {
      "epoch": 0.22083032201076047,
      "grad_norm": 0.03908105567097664,
      "learning_rate": 1.9117214058192138e-05,
      "loss": 0.0481,
      "step": 16500
    },
    {
      "epoch": 0.22752214995048048,
      "grad_norm": 0.055949293076992035,
      "learning_rate": 1.909044674643326e-05,
      "loss": 0.0484,
      "step": 17000
    },
    {
      "epoch": 0.2342139778902005,
      "grad_norm": 0.37154585123062134,
      "learning_rate": 1.906367943467438e-05,
      "loss": 0.0481,
      "step": 17500
    },
    {
      "epoch": 0.2409058058299205,
      "grad_norm": 6.115246772766113,
      "learning_rate": 1.9036965657539014e-05,
      "loss": 0.0607,
      "step": 18000
    },
    {
      "epoch": 0.2475976337696405,
      "grad_norm": 0.0021434242371469736,
      "learning_rate": 1.9010198345780134e-05,
      "loss": 0.0491,
      "step": 18500
    },
    {
      "epoch": 0.2542894617093605,
      "grad_norm": 0.0011307507520541549,
      "learning_rate": 1.8983431034021255e-05,
      "loss": 0.0406,
      "step": 19000
    },
    {
      "epoch": 0.2609812896490805,
      "grad_norm": 0.029870424419641495,
      "learning_rate": 1.8956663722262375e-05,
      "loss": 0.041,
      "step": 19500
    },
    {
      "epoch": 0.2676731175888006,
      "grad_norm": 0.15462139248847961,
      "learning_rate": 1.8929896410503495e-05,
      "loss": 0.0446,
      "step": 20000
    },
    {
      "epoch": 0.2743649455285206,
      "grad_norm": 21.894237518310547,
      "learning_rate": 1.8903182633368134e-05,
      "loss": 0.0418,
      "step": 20500
    },
    {
      "epoch": 0.2810567734682406,
      "grad_norm": 0.0015144560020416975,
      "learning_rate": 1.887641532160925e-05,
      "loss": 0.0508,
      "step": 21000
    },
    {
      "epoch": 0.2877486014079606,
      "grad_norm": 0.05776594951748848,
      "learning_rate": 1.884964800985037e-05,
      "loss": 0.0427,
      "step": 21500
    },
    {
      "epoch": 0.2944404293476806,
      "grad_norm": 0.04988189786672592,
      "learning_rate": 1.882288069809149e-05,
      "loss": 0.0347,
      "step": 22000
    },
    {
      "epoch": 0.3011322572874006,
      "grad_norm": 0.030231794342398643,
      "learning_rate": 1.879611338633261e-05,
      "loss": 0.0503,
      "step": 22500
    },
    {
      "epoch": 0.3078240852271206,
      "grad_norm": 0.10336820036172867,
      "learning_rate": 1.8769346074573732e-05,
      "loss": 0.0478,
      "step": 23000
    },
    {
      "epoch": 0.31451591316684063,
      "grad_norm": 0.00644893292337656,
      "learning_rate": 1.8742578762814852e-05,
      "loss": 0.0366,
      "step": 23500
    },
    {
      "epoch": 0.3212077411065607,
      "grad_norm": 0.05720679089426994,
      "learning_rate": 1.8715811451055973e-05,
      "loss": 0.0446,
      "step": 24000
    },
    {
      "epoch": 0.3278995690462807,
      "grad_norm": 0.016544243320822716,
      "learning_rate": 1.8689097673920608e-05,
      "loss": 0.0364,
      "step": 24500
    },
    {
      "epoch": 0.3345913969860007,
      "grad_norm": 0.11171264946460724,
      "learning_rate": 1.8662330362161728e-05,
      "loss": 0.0426,
      "step": 25000
    },
    {
      "epoch": 0.3412832249257207,
      "grad_norm": 0.00344551051966846,
      "learning_rate": 1.863556305040285e-05,
      "loss": 0.0387,
      "step": 25500
    },
    {
      "epoch": 0.3479750528654407,
      "grad_norm": 0.01038755290210247,
      "learning_rate": 1.860879573864397e-05,
      "loss": 0.0307,
      "step": 26000
    },
    {
      "epoch": 0.35466688080516073,
      "grad_norm": 0.01718481257557869,
      "learning_rate": 1.858202842688509e-05,
      "loss": 0.04,
      "step": 26500
    },
    {
      "epoch": 0.36135870874488074,
      "grad_norm": 0.011422321200370789,
      "learning_rate": 1.855526111512621e-05,
      "loss": 0.0401,
      "step": 27000
    },
    {
      "epoch": 0.36805053668460075,
      "grad_norm": 0.10280390083789825,
      "learning_rate": 1.852849380336733e-05,
      "loss": 0.0378,
      "step": 27500
    },
    {
      "epoch": 0.37474236462432076,
      "grad_norm": 0.00888754427433014,
      "learning_rate": 1.850172649160845e-05,
      "loss": 0.0297,
      "step": 28000
    },
    {
      "epoch": 0.3814341925640408,
      "grad_norm": 0.002325992565602064,
      "learning_rate": 1.8475012714473085e-05,
      "loss": 0.0437,
      "step": 28500
    },
    {
      "epoch": 0.3881260205037608,
      "grad_norm": 0.001831004861742258,
      "learning_rate": 1.8448245402714205e-05,
      "loss": 0.0411,
      "step": 29000
    },
    {
      "epoch": 0.39481784844348083,
      "grad_norm": 0.0030924235470592976,
      "learning_rate": 1.8421478090955326e-05,
      "loss": 0.0463,
      "step": 29500
    },
    {
      "epoch": 0.40150967638320084,
      "grad_norm": 0.0014463290572166443,
      "learning_rate": 1.8394710779196446e-05,
      "loss": 0.032,
      "step": 30000
    },
    {
      "epoch": 0.40820150432292085,
      "grad_norm": 0.01593749038875103,
      "learning_rate": 1.8367943467437566e-05,
      "loss": 0.0436,
      "step": 30500
    },
    {
      "epoch": 0.41489333226264086,
      "grad_norm": 0.0017704571364447474,
      "learning_rate": 1.8341229690302205e-05,
      "loss": 0.0243,
      "step": 31000
    },
    {
      "epoch": 0.42158516020236086,
      "grad_norm": 0.019163450226187706,
      "learning_rate": 1.8314462378543325e-05,
      "loss": 0.0399,
      "step": 31500
    },
    {
      "epoch": 0.42827698814208087,
      "grad_norm": 0.01753789372742176,
      "learning_rate": 1.8287695066784442e-05,
      "loss": 0.0475,
      "step": 32000
    },
    {
      "epoch": 0.4349688160818009,
      "grad_norm": 0.023159803822636604,
      "learning_rate": 1.8260927755025563e-05,
      "loss": 0.0446,
      "step": 32500
    },
    {
      "epoch": 0.44166064402152094,
      "grad_norm": 0.003751636715605855,
      "learning_rate": 1.8234160443266683e-05,
      "loss": 0.047,
      "step": 33000
    },
    {
      "epoch": 0.44835247196124095,
      "grad_norm": 0.04879254102706909,
      "learning_rate": 1.8207393131507803e-05,
      "loss": 0.0359,
      "step": 33500
    },
    {
      "epoch": 0.45504429990096096,
      "grad_norm": 0.03544230759143829,
      "learning_rate": 1.8180625819748923e-05,
      "loss": 0.0415,
      "step": 34000
    },
    {
      "epoch": 0.46173612784068097,
      "grad_norm": 0.008522383868694305,
      "learning_rate": 1.8153858507990044e-05,
      "loss": 0.0354,
      "step": 34500
    },
    {
      "epoch": 0.468427955780401,
      "grad_norm": 0.1573578417301178,
      "learning_rate": 1.8127091196231164e-05,
      "loss": 0.0388,
      "step": 35000
    },
    {
      "epoch": 0.475119783720121,
      "grad_norm": 0.0008812965243123472,
      "learning_rate": 1.8100323884472284e-05,
      "loss": 0.0321,
      "step": 35500
    },
    {
      "epoch": 0.481811611659841,
      "grad_norm": 0.042298343032598495,
      "learning_rate": 1.8073556572713405e-05,
      "loss": 0.03,
      "step": 36000
    },
    {
      "epoch": 0.488503439599561,
      "grad_norm": 0.004292197059839964,
      "learning_rate": 1.8046789260954525e-05,
      "loss": 0.021,
      "step": 36500
    },
    {
      "epoch": 0.495195267539281,
      "grad_norm": 0.012195637449622154,
      "learning_rate": 1.8020021949195642e-05,
      "loss": 0.0379,
      "step": 37000
    },
    {
      "epoch": 0.501887095479001,
      "grad_norm": 0.003010038286447525,
      "learning_rate": 1.7993254637436762e-05,
      "loss": 0.0356,
      "step": 37500
    },
    {
      "epoch": 0.508578923418721,
      "grad_norm": 0.12932276725769043,
      "learning_rate": 1.796659439492492e-05,
      "loss": 0.037,
      "step": 38000
    },
    {
      "epoch": 0.515270751358441,
      "grad_norm": 0.0034719156101346016,
      "learning_rate": 1.793982708316604e-05,
      "loss": 0.0275,
      "step": 38500
    },
    {
      "epoch": 0.521962579298161,
      "grad_norm": 1.3697911500930786,
      "learning_rate": 1.791305977140716e-05,
      "loss": 0.0359,
      "step": 39000
    },
    {
      "epoch": 0.5286544072378812,
      "grad_norm": 0.03641897067427635,
      "learning_rate": 1.788629245964828e-05,
      "loss": 0.0314,
      "step": 39500
    },
    {
      "epoch": 0.5353462351776012,
      "grad_norm": 0.07218953222036362,
      "learning_rate": 1.7859525147889397e-05,
      "loss": 0.0354,
      "step": 40000
    },
    {
      "epoch": 0.5420380631173212,
      "grad_norm": 0.0013539899373427033,
      "learning_rate": 1.783281137075404e-05,
      "loss": 0.0323,
      "step": 40500
    },
    {
      "epoch": 0.5487298910570412,
      "grad_norm": 0.007444677408784628,
      "learning_rate": 1.7806044058995156e-05,
      "loss": 0.0317,
      "step": 41000
    },
    {
      "epoch": 0.5554217189967612,
      "grad_norm": 0.09833970665931702,
      "learning_rate": 1.7779276747236276e-05,
      "loss": 0.0364,
      "step": 41500
    },
    {
      "epoch": 0.5621135469364812,
      "grad_norm": 0.008745625615119934,
      "learning_rate": 1.7752509435477397e-05,
      "loss": 0.0219,
      "step": 42000
    },
    {
      "epoch": 0.5688053748762012,
      "grad_norm": 0.013100198470056057,
      "learning_rate": 1.7725742123718517e-05,
      "loss": 0.045,
      "step": 42500
    },
    {
      "epoch": 0.5754972028159212,
      "grad_norm": 0.009543226100504398,
      "learning_rate": 1.7698974811959637e-05,
      "loss": 0.0338,
      "step": 43000
    },
    {
      "epoch": 0.5821890307556412,
      "grad_norm": 0.154267355799675,
      "learning_rate": 1.7672261034824272e-05,
      "loss": 0.0485,
      "step": 43500
    },
    {
      "epoch": 0.5888808586953612,
      "grad_norm": 0.00789195578545332,
      "learning_rate": 1.764554725768891e-05,
      "loss": 0.0528,
      "step": 44000
    },
    {
      "epoch": 0.5955726866350812,
      "grad_norm": 0.004760543815791607,
      "learning_rate": 1.761877994593003e-05,
      "loss": 0.0335,
      "step": 44500
    },
    {
      "epoch": 0.6022645145748012,
      "grad_norm": 0.006895027589052916,
      "learning_rate": 1.759201263417115e-05,
      "loss": 0.0385,
      "step": 45000
    },
    {
      "epoch": 0.6089563425145212,
      "grad_norm": 29.424293518066406,
      "learning_rate": 1.7565245322412272e-05,
      "loss": 0.0975,
      "step": 45500
    },
    {
      "epoch": 0.6156481704542413,
      "grad_norm": 0.14066633582115173,
      "learning_rate": 1.7538478010653392e-05,
      "loss": 0.0401,
      "step": 46000
    },
    {
      "epoch": 0.6223399983939613,
      "grad_norm": 11.78432559967041,
      "learning_rate": 1.751171069889451e-05,
      "loss": 0.034,
      "step": 46500
    },
    {
      "epoch": 0.6290318263336813,
      "grad_norm": 3.980072021484375,
      "learning_rate": 1.748494338713563e-05,
      "loss": 0.0352,
      "step": 47000
    },
    {
      "epoch": 0.6357236542734013,
      "grad_norm": 0.005017422139644623,
      "learning_rate": 1.7458176075376753e-05,
      "loss": 0.0367,
      "step": 47500
    },
    {
      "epoch": 0.6424154822131214,
      "grad_norm": 0.018190061673521996,
      "learning_rate": 1.7431408763617873e-05,
      "loss": 0.0455,
      "step": 48000
    },
    {
      "epoch": 0.6491073101528414,
      "grad_norm": 0.0690702423453331,
      "learning_rate": 1.740469498648251e-05,
      "loss": 0.0439,
      "step": 48500
    },
    {
      "epoch": 0.6557991380925614,
      "grad_norm": 0.02622833661735058,
      "learning_rate": 1.737792767472363e-05,
      "loss": 0.0534,
      "step": 49000
    },
    {
      "epoch": 0.6624909660322814,
      "grad_norm": 0.007410981692373753,
      "learning_rate": 1.7351213897588268e-05,
      "loss": 0.0565,
      "step": 49500
    },
    {
      "epoch": 0.6691827939720014,
      "grad_norm": 0.023615024983882904,
      "learning_rate": 1.7324446585829388e-05,
      "loss": 0.0303,
      "step": 50000
    },
    {
      "epoch": 0.6758746219117214,
      "grad_norm": 25.32433319091797,
      "learning_rate": 1.7297732808694023e-05,
      "loss": 0.0354,
      "step": 50500
    },
    {
      "epoch": 0.6825664498514414,
      "grad_norm": 0.016241949051618576,
      "learning_rate": 1.7270965496935144e-05,
      "loss": 0.0522,
      "step": 51000
    },
    {
      "epoch": 0.6892582777911614,
      "grad_norm": 0.02320280112326145,
      "learning_rate": 1.7244198185176264e-05,
      "loss": 0.0315,
      "step": 51500
    },
    {
      "epoch": 0.6959501057308815,
      "grad_norm": 0.14172767102718353,
      "learning_rate": 1.7217430873417384e-05,
      "loss": 0.0361,
      "step": 52000
    },
    {
      "epoch": 0.7026419336706015,
      "grad_norm": 0.0428684763610363,
      "learning_rate": 1.7190663561658504e-05,
      "loss": 0.0504,
      "step": 52500
    },
    {
      "epoch": 0.7093337616103215,
      "grad_norm": 0.006326210219413042,
      "learning_rate": 1.7163896249899625e-05,
      "loss": 0.0396,
      "step": 53000
    },
    {
      "epoch": 0.7160255895500415,
      "grad_norm": 0.006577476859092712,
      "learning_rate": 1.7137128938140745e-05,
      "loss": 0.0414,
      "step": 53500
    },
    {
      "epoch": 0.7227174174897615,
      "grad_norm": 0.005494531709700823,
      "learning_rate": 1.7110415161005384e-05,
      "loss": 0.0397,
      "step": 54000
    },
    {
      "epoch": 0.7294092454294815,
      "grad_norm": 0.051431700587272644,
      "learning_rate": 1.708370138387002e-05,
      "loss": 0.0369,
      "step": 54500
    },
    {
      "epoch": 0.7361010733692015,
      "grad_norm": 0.03829536586999893,
      "learning_rate": 1.705693407211114e-05,
      "loss": 0.0581,
      "step": 55000
    },
    {
      "epoch": 0.7427929013089215,
      "grad_norm": 0.015044815838336945,
      "learning_rate": 1.703016676035226e-05,
      "loss": 0.0415,
      "step": 55500
    },
    {
      "epoch": 0.7494847292486415,
      "grad_norm": 0.024291738867759705,
      "learning_rate": 1.700339944859338e-05,
      "loss": 0.0393,
      "step": 56000
    },
    {
      "epoch": 0.7561765571883615,
      "grad_norm": 0.008540622889995575,
      "learning_rate": 1.69766321368345e-05,
      "loss": 0.0343,
      "step": 56500
    },
    {
      "epoch": 0.7628683851280816,
      "grad_norm": 0.040114279836416245,
      "learning_rate": 1.694986482507562e-05,
      "loss": 0.0434,
      "step": 57000
    },
    {
      "epoch": 0.7695602130678016,
      "grad_norm": 0.015358100645244122,
      "learning_rate": 1.692309751331674e-05,
      "loss": 0.0392,
      "step": 57500
    },
    {
      "epoch": 0.7762520410075217,
      "grad_norm": 4.507519245147705,
      "learning_rate": 1.6896330201557858e-05,
      "loss": 0.0288,
      "step": 58000
    },
    {
      "epoch": 0.7829438689472417,
      "grad_norm": 0.007302429527044296,
      "learning_rate": 1.6869562889798978e-05,
      "loss": 0.037,
      "step": 58500
    },
    {
      "epoch": 0.7896356968869617,
      "grad_norm": 0.027282489463686943,
      "learning_rate": 1.6842795578040098e-05,
      "loss": 0.0505,
      "step": 59000
    },
    {
      "epoch": 0.7963275248266817,
      "grad_norm": 0.0435088686645031,
      "learning_rate": 1.681602826628122e-05,
      "loss": 0.0372,
      "step": 59500
    },
    {
      "epoch": 0.8030193527664017,
      "grad_norm": 0.004382804036140442,
      "learning_rate": 1.678926095452234e-05,
      "loss": 0.041,
      "step": 60000
    },
    {
      "epoch": 0.8097111807061217,
      "grad_norm": 0.0889260470867157,
      "learning_rate": 1.676249364276346e-05,
      "loss": 0.0392,
      "step": 60500
    },
    {
      "epoch": 0.8164030086458417,
      "grad_norm": 0.6129966974258423,
      "learning_rate": 1.673572633100458e-05,
      "loss": 0.0457,
      "step": 61000
    },
    {
      "epoch": 0.8230948365855617,
      "grad_norm": 0.12050234526395798,
      "learning_rate": 1.67089590192457e-05,
      "loss": 0.0393,
      "step": 61500
    },
    {
      "epoch": 0.8297866645252817,
      "grad_norm": 0.023022426292300224,
      "learning_rate": 1.6682245242110335e-05,
      "loss": 0.0381,
      "step": 62000
    },
    {
      "epoch": 0.8364784924650017,
      "grad_norm": 0.12084700167179108,
      "learning_rate": 1.6655477930351455e-05,
      "loss": 0.04,
      "step": 62500
    },
    {
      "epoch": 0.8431703204047217,
      "grad_norm": 0.06323599070310593,
      "learning_rate": 1.6628764153216094e-05,
      "loss": 0.0464,
      "step": 63000
    },
    {
      "epoch": 0.8498621483444417,
      "grad_norm": 0.056267641484737396,
      "learning_rate": 1.6601996841457214e-05,
      "loss": 0.0309,
      "step": 63500
    },
    {
      "epoch": 0.8565539762841617,
      "grad_norm": 0.13685142993927002,
      "learning_rate": 1.6575229529698335e-05,
      "loss": 0.0382,
      "step": 64000
    },
    {
      "epoch": 0.8632458042238818,
      "grad_norm": 0.01324182003736496,
      "learning_rate": 1.6548462217939455e-05,
      "loss": 0.048,
      "step": 64500
    },
    {
      "epoch": 0.8699376321636018,
      "grad_norm": 25.4251766204834,
      "learning_rate": 1.652174844080409e-05,
      "loss": 0.0622,
      "step": 65000
    },
    {
      "epoch": 0.8766294601033219,
      "grad_norm": 0.019407005980610847,
      "learning_rate": 1.649498112904521e-05,
      "loss": 0.034,
      "step": 65500
    },
    {
      "epoch": 0.8833212880430419,
      "grad_norm": 0.013158817775547504,
      "learning_rate": 1.646821381728633e-05,
      "loss": 0.04,
      "step": 66000
    },
    {
      "epoch": 0.8900131159827619,
      "grad_norm": 0.022213362157344818,
      "learning_rate": 1.644144650552745e-05,
      "loss": 0.0424,
      "step": 66500
    },
    {
      "epoch": 0.8967049439224819,
      "grad_norm": 0.016293920576572418,
      "learning_rate": 1.641467919376857e-05,
      "loss": 0.0441,
      "step": 67000
    },
    {
      "epoch": 0.9033967718622019,
      "grad_norm": 0.11932259798049927,
      "learning_rate": 1.6387911882009692e-05,
      "loss": 0.0444,
      "step": 67500
    },
    {
      "epoch": 0.9100885998019219,
      "grad_norm": 0.03209853172302246,
      "learning_rate": 1.636119810487433e-05,
      "loss": 0.0358,
      "step": 68000
    },
    {
      "epoch": 0.9167804277416419,
      "grad_norm": 0.03486077859997749,
      "learning_rate": 1.6334430793115447e-05,
      "loss": 0.0389,
      "step": 68500
    },
    {
      "epoch": 0.9234722556813619,
      "grad_norm": 0.015240680426359177,
      "learning_rate": 1.6307663481356568e-05,
      "loss": 0.0446,
      "step": 69000
    },
    {
      "epoch": 0.9301640836210819,
      "grad_norm": 0.015254886820912361,
      "learning_rate": 1.6280896169597688e-05,
      "loss": 0.0682,
      "step": 69500
    },
    {
      "epoch": 0.936855911560802,
      "grad_norm": 3.8672003746032715,
      "learning_rate": 1.6254128857838808e-05,
      "loss": 0.0895,
      "step": 70000
    },
    {
      "epoch": 0.943547739500522,
      "grad_norm": 42.00361633300781,
      "learning_rate": 1.622736154607993e-05,
      "loss": 0.0785,
      "step": 70500
    },
    {
      "epoch": 0.950239567440242,
      "grad_norm": 0.017758021131157875,
      "learning_rate": 1.620059423432105e-05,
      "loss": 0.0711,
      "step": 71000
    },
    {
      "epoch": 0.956931395379962,
      "grad_norm": 0.020184917375445366,
      "learning_rate": 1.617382692256217e-05,
      "loss": 0.0543,
      "step": 71500
    },
    {
      "epoch": 0.963623223319682,
      "grad_norm": 0.018102100118994713,
      "learning_rate": 1.614705961080329e-05,
      "loss": 0.0491,
      "step": 72000
    },
    {
      "epoch": 0.970315051259402,
      "grad_norm": 0.02041660249233246,
      "learning_rate": 1.612029229904441e-05,
      "loss": 0.0601,
      "step": 72500
    },
    {
      "epoch": 0.977006879199122,
      "grad_norm": 0.016586367040872574,
      "learning_rate": 1.609352498728553e-05,
      "loss": 0.0468,
      "step": 73000
    },
    {
      "epoch": 0.983698707138842,
      "grad_norm": 0.010597994551062584,
      "learning_rate": 1.6066757675526647e-05,
      "loss": 0.0299,
      "step": 73500
    },
    {
      "epoch": 0.990390535078562,
      "grad_norm": 0.033980842679739,
      "learning_rate": 1.6039990363767767e-05,
      "loss": 0.0607,
      "step": 74000
    },
    {
      "epoch": 0.9970823630182821,
      "grad_norm": 0.016405552625656128,
      "learning_rate": 1.6013276586632406e-05,
      "loss": 0.0406,
      "step": 74500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9840065312240691,
      "eval_loss": 0.07214654982089996,
      "eval_runtime": 143.2855,
      "eval_samples_per_second": 521.462,
      "eval_steps_per_second": 65.185,
      "step": 74718
    },
    {
      "epoch": 1.003774190958002,
      "grad_norm": 0.02102821134030819,
      "learning_rate": 1.5986509274873526e-05,
      "loss": 0.0569,
      "step": 75000
    },
    {
      "epoch": 1.0104660188977221,
      "grad_norm": 0.024627545848488808,
      "learning_rate": 1.5959741963114646e-05,
      "loss": 0.0712,
      "step": 75500
    },
    {
      "epoch": 1.017157846837442,
      "grad_norm": 11.847190856933594,
      "learning_rate": 1.5932974651355767e-05,
      "loss": 0.0505,
      "step": 76000
    },
    {
      "epoch": 1.0238496747771622,
      "grad_norm": 0.017919247969985008,
      "learning_rate": 1.5906260874220402e-05,
      "loss": 0.042,
      "step": 76500
    },
    {
      "epoch": 1.030541502716882,
      "grad_norm": 0.01854877360165119,
      "learning_rate": 1.5879493562461522e-05,
      "loss": 0.0433,
      "step": 77000
    },
    {
      "epoch": 1.0372333306566022,
      "grad_norm": 0.01307060569524765,
      "learning_rate": 1.5852726250702643e-05,
      "loss": 0.0447,
      "step": 77500
    },
    {
      "epoch": 1.043925158596322,
      "grad_norm": 89.74649047851562,
      "learning_rate": 1.5825958938943763e-05,
      "loss": 0.0456,
      "step": 78000
    },
    {
      "epoch": 1.0506169865360422,
      "grad_norm": 0.01767083816230297,
      "learning_rate": 1.5799191627184883e-05,
      "loss": 0.0354,
      "step": 78500
    },
    {
      "epoch": 1.0573088144757623,
      "grad_norm": 26.644878387451172,
      "learning_rate": 1.5772477850049522e-05,
      "loss": 0.0559,
      "step": 79000
    },
    {
      "epoch": 1.0640006424154822,
      "grad_norm": 0.0608225092291832,
      "learning_rate": 1.5745710538290642e-05,
      "loss": 0.0326,
      "step": 79500
    },
    {
      "epoch": 1.0706924703552023,
      "grad_norm": 0.020465107634663582,
      "learning_rate": 1.571894322653176e-05,
      "loss": 0.0504,
      "step": 80000
    },
    {
      "epoch": 1.0773842982949222,
      "grad_norm": 0.023333575576543808,
      "learning_rate": 1.569217591477288e-05,
      "loss": 0.0331,
      "step": 80500
    },
    {
      "epoch": 1.0840761262346423,
      "grad_norm": 0.24663817882537842,
      "learning_rate": 1.5665408603014e-05,
      "loss": 0.0383,
      "step": 81000
    },
    {
      "epoch": 1.0907679541743622,
      "grad_norm": 0.005013224668800831,
      "learning_rate": 1.5638748360502157e-05,
      "loss": 0.0435,
      "step": 81500
    },
    {
      "epoch": 1.0974597821140823,
      "grad_norm": 0.013139571994543076,
      "learning_rate": 1.5612034583366792e-05,
      "loss": 0.0486,
      "step": 82000
    },
    {
      "epoch": 1.1041516100538022,
      "grad_norm": 0.01658661849796772,
      "learning_rate": 1.5585267271607912e-05,
      "loss": 0.0254,
      "step": 82500
    },
    {
      "epoch": 1.1108434379935224,
      "grad_norm": 0.09269557148218155,
      "learning_rate": 1.5558499959849033e-05,
      "loss": 0.0376,
      "step": 83000
    },
    {
      "epoch": 1.1175352659332423,
      "grad_norm": 0.19779546558856964,
      "learning_rate": 1.5531732648090153e-05,
      "loss": 0.1053,
      "step": 83500
    },
    {
      "epoch": 1.1242270938729624,
      "grad_norm": 0.030784372240304947,
      "learning_rate": 1.5504965336331273e-05,
      "loss": 0.0508,
      "step": 84000
    },
    {
      "epoch": 1.1309189218126823,
      "grad_norm": 0.06557115167379379,
      "learning_rate": 1.5478198024572393e-05,
      "loss": 0.0476,
      "step": 84500
    },
    {
      "epoch": 1.1376107497524024,
      "grad_norm": 0.01329509261995554,
      "learning_rate": 1.5451430712813514e-05,
      "loss": 0.0363,
      "step": 85000
    },
    {
      "epoch": 1.1443025776921223,
      "grad_norm": 0.009438504464924335,
      "learning_rate": 1.5424663401054634e-05,
      "loss": 0.0266,
      "step": 85500
    },
    {
      "epoch": 1.1509944056318424,
      "grad_norm": 0.04822155833244324,
      "learning_rate": 1.5397896089295754e-05,
      "loss": 0.0376,
      "step": 86000
    },
    {
      "epoch": 1.1576862335715625,
      "grad_norm": 0.04102310910820961,
      "learning_rate": 1.537118231216039e-05,
      "loss": 0.0405,
      "step": 86500
    },
    {
      "epoch": 1.1643780615112824,
      "grad_norm": 0.039247360080480576,
      "learning_rate": 1.534441500040151e-05,
      "loss": 0.0667,
      "step": 87000
    },
    {
      "epoch": 1.1710698894510023,
      "grad_norm": 0.038806553930044174,
      "learning_rate": 1.531764768864263e-05,
      "loss": 0.0377,
      "step": 87500
    },
    {
      "epoch": 1.1777617173907224,
      "grad_norm": 0.030093537643551826,
      "learning_rate": 1.529088037688375e-05,
      "loss": 0.0366,
      "step": 88000
    },
    {
      "epoch": 1.1844535453304426,
      "grad_norm": 0.0578249990940094,
      "learning_rate": 1.526411306512487e-05,
      "loss": 0.0461,
      "step": 88500
    },
    {
      "epoch": 1.1911453732701625,
      "grad_norm": 0.010320750065147877,
      "learning_rate": 1.5237345753365991e-05,
      "loss": 0.0296,
      "step": 89000
    },
    {
      "epoch": 1.1978372012098826,
      "grad_norm": 0.043146319687366486,
      "learning_rate": 1.5210631976230628e-05,
      "loss": 0.0391,
      "step": 89500
    },
    {
      "epoch": 1.2045290291496025,
      "grad_norm": 0.02805778756737709,
      "learning_rate": 1.5183864664471748e-05,
      "loss": 0.0588,
      "step": 90000
    },
    {
      "epoch": 1.2112208570893226,
      "grad_norm": 0.01797872595489025,
      "learning_rate": 1.5157097352712869e-05,
      "loss": 0.0318,
      "step": 90500
    },
    {
      "epoch": 1.2179126850290425,
      "grad_norm": 0.06816426664590836,
      "learning_rate": 1.5130330040953989e-05,
      "loss": 0.0417,
      "step": 91000
    },
    {
      "epoch": 1.2246045129687626,
      "grad_norm": 0.021450074389576912,
      "learning_rate": 1.510356272919511e-05,
      "loss": 0.0456,
      "step": 91500
    },
    {
      "epoch": 1.2312963409084825,
      "grad_norm": 0.007763643283396959,
      "learning_rate": 1.5076795417436228e-05,
      "loss": 0.0362,
      "step": 92000
    },
    {
      "epoch": 1.2379881688482026,
      "grad_norm": 0.01165120117366314,
      "learning_rate": 1.5050028105677348e-05,
      "loss": 0.0392,
      "step": 92500
    },
    {
      "epoch": 1.2446799967879225,
      "grad_norm": 0.013199067674577236,
      "learning_rate": 1.5023260793918469e-05,
      "loss": 0.0324,
      "step": 93000
    },
    {
      "epoch": 1.2513718247276426,
      "grad_norm": 0.05698908492922783,
      "learning_rate": 1.4996493482159589e-05,
      "loss": 0.039,
      "step": 93500
    },
    {
      "epoch": 1.2580636526673628,
      "grad_norm": 7.417758464813232,
      "learning_rate": 1.4969726170400707e-05,
      "loss": 0.0382,
      "step": 94000
    },
    {
      "epoch": 1.2647554806070826,
      "grad_norm": 0.014565900899469852,
      "learning_rate": 1.4942958858641828e-05,
      "loss": 0.0392,
      "step": 94500
    },
    {
      "epoch": 1.2714473085468025,
      "grad_norm": 0.0052986349910497665,
      "learning_rate": 1.4916245081506466e-05,
      "loss": 0.0228,
      "step": 95000
    },
    {
      "epoch": 1.2781391364865227,
      "grad_norm": 0.03638508543372154,
      "learning_rate": 1.4889477769747585e-05,
      "loss": 0.0386,
      "step": 95500
    },
    {
      "epoch": 1.2848309644262428,
      "grad_norm": 0.00800937321037054,
      "learning_rate": 1.4862710457988705e-05,
      "loss": 0.0311,
      "step": 96000
    },
    {
      "epoch": 1.2915227923659627,
      "grad_norm": 0.013006594032049179,
      "learning_rate": 1.4835943146229826e-05,
      "loss": 0.0364,
      "step": 96500
    },
    {
      "epoch": 1.2982146203056826,
      "grad_norm": 0.02121400274336338,
      "learning_rate": 1.4809175834470946e-05,
      "loss": 0.0381,
      "step": 97000
    },
    {
      "epoch": 1.3049064482454027,
      "grad_norm": 0.017525535076856613,
      "learning_rate": 1.4782408522712066e-05,
      "loss": 0.0429,
      "step": 97500
    },
    {
      "epoch": 1.3115982761851228,
      "grad_norm": 0.019446944817900658,
      "learning_rate": 1.4755694745576703e-05,
      "loss": 0.0337,
      "step": 98000
    },
    {
      "epoch": 1.3182901041248427,
      "grad_norm": 3.649751901626587,
      "learning_rate": 1.4728927433817823e-05,
      "loss": 0.0344,
      "step": 98500
    },
    {
      "epoch": 1.3249819320645628,
      "grad_norm": 5.589725971221924,
      "learning_rate": 1.4702160122058944e-05,
      "loss": 0.03,
      "step": 99000
    },
    {
      "epoch": 1.3316737600042827,
      "grad_norm": 0.017418405041098595,
      "learning_rate": 1.4675392810300062e-05,
      "loss": 0.0341,
      "step": 99500
    },
    {
      "epoch": 1.3383655879440028,
      "grad_norm": 0.029940037056803703,
      "learning_rate": 1.4648679033164701e-05,
      "loss": 0.0493,
      "step": 100000
    },
    {
      "epoch": 1.3450574158837227,
      "grad_norm": 0.22235819697380066,
      "learning_rate": 1.4621911721405821e-05,
      "loss": 0.059,
      "step": 100500
    },
    {
      "epoch": 1.3517492438234429,
      "grad_norm": 0.07213294506072998,
      "learning_rate": 1.459514440964694e-05,
      "loss": 0.0427,
      "step": 101000
    },
    {
      "epoch": 1.3584410717631628,
      "grad_norm": 0.017565639689564705,
      "learning_rate": 1.456837709788806e-05,
      "loss": 0.0279,
      "step": 101500
    },
    {
      "epoch": 1.3651328997028829,
      "grad_norm": 0.016205454245209694,
      "learning_rate": 1.454160978612918e-05,
      "loss": 0.0447,
      "step": 102000
    },
    {
      "epoch": 1.3718247276426028,
      "grad_norm": 0.02381316013634205,
      "learning_rate": 1.4514896008993817e-05,
      "loss": 0.0449,
      "step": 102500
    },
    {
      "epoch": 1.3785165555823229,
      "grad_norm": 0.03551754727959633,
      "learning_rate": 1.4488128697234938e-05,
      "loss": 0.035,
      "step": 103000
    },
    {
      "epoch": 1.385208383522043,
      "grad_norm": 0.12012482434511185,
      "learning_rate": 1.4461361385476058e-05,
      "loss": 0.055,
      "step": 103500
    },
    {
      "epoch": 1.391900211461763,
      "grad_norm": 0.02220664918422699,
      "learning_rate": 1.4434594073717178e-05,
      "loss": 0.0403,
      "step": 104000
    },
    {
      "epoch": 1.3985920394014828,
      "grad_norm": 0.03510086610913277,
      "learning_rate": 1.4407826761958297e-05,
      "loss": 0.0644,
      "step": 104500
    },
    {
      "epoch": 1.405283867341203,
      "grad_norm": 0.018089327961206436,
      "learning_rate": 1.4381059450199417e-05,
      "loss": 0.0422,
      "step": 105000
    },
    {
      "epoch": 1.411975695280923,
      "grad_norm": 0.01667984575033188,
      "learning_rate": 1.4354292138440538e-05,
      "loss": 0.0463,
      "step": 105500
    },
    {
      "epoch": 1.418667523220643,
      "grad_norm": 0.011023368686437607,
      "learning_rate": 1.4327578361305175e-05,
      "loss": 0.0547,
      "step": 106000
    },
    {
      "epoch": 1.4253593511603628,
      "grad_norm": 0.017395727336406708,
      "learning_rate": 1.4300864584169813e-05,
      "loss": 0.0415,
      "step": 106500
    },
    {
      "epoch": 1.432051179100083,
      "grad_norm": 8.840470314025879,
      "learning_rate": 1.4274097272410933e-05,
      "loss": 0.0336,
      "step": 107000
    },
    {
      "epoch": 1.438743007039803,
      "grad_norm": 0.03199724480509758,
      "learning_rate": 1.4247329960652052e-05,
      "loss": 0.0453,
      "step": 107500
    },
    {
      "epoch": 1.445434834979523,
      "grad_norm": 0.04399611055850983,
      "learning_rate": 1.4220562648893172e-05,
      "loss": 0.0585,
      "step": 108000
    },
    {
      "epoch": 1.452126662919243,
      "grad_norm": 0.0176805779337883,
      "learning_rate": 1.4193795337134293e-05,
      "loss": 0.0331,
      "step": 108500
    },
    {
      "epoch": 1.458818490858963,
      "grad_norm": 3.731849193572998,
      "learning_rate": 1.4167028025375413e-05,
      "loss": 0.0591,
      "step": 109000
    },
    {
      "epoch": 1.465510318798683,
      "grad_norm": 0.022814054042100906,
      "learning_rate": 1.4140260713616533e-05,
      "loss": 0.0452,
      "step": 109500
    },
    {
      "epoch": 1.472202146738403,
      "grad_norm": 7.446138381958008,
      "learning_rate": 1.411354693648117e-05,
      "loss": 0.0564,
      "step": 110000
    },
    {
      "epoch": 1.4788939746781231,
      "grad_norm": 0.025479143485426903,
      "learning_rate": 1.408677962472229e-05,
      "loss": 0.043,
      "step": 110500
    },
    {
      "epoch": 1.485585802617843,
      "grad_norm": 0.056819166988134384,
      "learning_rate": 1.406001231296341e-05,
      "loss": 0.039,
      "step": 111000
    },
    {
      "epoch": 1.4922776305575631,
      "grad_norm": 3.6375038623809814,
      "learning_rate": 1.403324500120453e-05,
      "loss": 0.0545,
      "step": 111500
    },
    {
      "epoch": 1.498969458497283,
      "grad_norm": 0.06564651429653168,
      "learning_rate": 1.400647768944565e-05,
      "loss": 0.0282,
      "step": 112000
    },
    {
      "epoch": 1.5056612864370031,
      "grad_norm": 0.06722092628479004,
      "learning_rate": 1.397971037768677e-05,
      "loss": 0.0472,
      "step": 112500
    },
    {
      "epoch": 1.5123531143767233,
      "grad_norm": 0.07936404645442963,
      "learning_rate": 1.395294306592789e-05,
      "loss": 0.085,
      "step": 113000
    },
    {
      "epoch": 1.5190449423164432,
      "grad_norm": 0.037150606513023376,
      "learning_rate": 1.3926175754169009e-05,
      "loss": 0.0463,
      "step": 113500
    },
    {
      "epoch": 1.525736770256163,
      "grad_norm": 0.03264281898736954,
      "learning_rate": 1.389940844241013e-05,
      "loss": 0.0331,
      "step": 114000
    },
    {
      "epoch": 1.5324285981958832,
      "grad_norm": 0.09330461919307709,
      "learning_rate": 1.387264113065125e-05,
      "loss": 0.0452,
      "step": 114500
    },
    {
      "epoch": 1.5391204261356033,
      "grad_norm": 0.050791043788194656,
      "learning_rate": 1.384587381889237e-05,
      "loss": 0.0744,
      "step": 115000
    },
    {
      "epoch": 1.5458122540753232,
      "grad_norm": 0.028209231793880463,
      "learning_rate": 1.381910650713349e-05,
      "loss": 0.0398,
      "step": 115500
    },
    {
      "epoch": 1.552504082015043,
      "grad_norm": 0.0324074812233448,
      "learning_rate": 1.3792339195374609e-05,
      "loss": 0.0481,
      "step": 116000
    },
    {
      "epoch": 1.5591959099547632,
      "grad_norm": 3.863939046859741,
      "learning_rate": 1.3765625418239247e-05,
      "loss": 0.0554,
      "step": 116500
    },
    {
      "epoch": 1.5658877378944833,
      "grad_norm": 0.05868634581565857,
      "learning_rate": 1.3738858106480368e-05,
      "loss": 0.0633,
      "step": 117000
    },
    {
      "epoch": 1.5725795658342032,
      "grad_norm": 0.037994105368852615,
      "learning_rate": 1.3712090794721486e-05,
      "loss": 0.0414,
      "step": 117500
    },
    {
      "epoch": 1.5792713937739233,
      "grad_norm": 4.7385382652282715,
      "learning_rate": 1.3685323482962607e-05,
      "loss": 0.0468,
      "step": 118000
    },
    {
      "epoch": 1.5859632217136435,
      "grad_norm": 0.03836880624294281,
      "learning_rate": 1.3658556171203727e-05,
      "loss": 0.0427,
      "step": 118500
    },
    {
      "epoch": 1.5926550496533634,
      "grad_norm": 0.01194749865680933,
      "learning_rate": 1.3631788859444847e-05,
      "loss": 0.0209,
      "step": 119000
    },
    {
      "epoch": 1.5993468775930832,
      "grad_norm": 0.018777653574943542,
      "learning_rate": 1.3605021547685966e-05,
      "loss": 0.0439,
      "step": 119500
    },
    {
      "epoch": 1.6060387055328034,
      "grad_norm": 0.04356293007731438,
      "learning_rate": 1.3578254235927086e-05,
      "loss": 0.035,
      "step": 120000
    },
    {
      "epoch": 1.6127305334725235,
      "grad_norm": 0.004735275637358427,
      "learning_rate": 1.3551486924168207e-05,
      "loss": 0.0243,
      "step": 120500
    },
    {
      "epoch": 1.6194223614122434,
      "grad_norm": 0.015763241797685623,
      "learning_rate": 1.3524719612409327e-05,
      "loss": 0.0343,
      "step": 121000
    },
    {
      "epoch": 1.6261141893519633,
      "grad_norm": 0.020778896287083626,
      "learning_rate": 1.3497952300650447e-05,
      "loss": 0.0386,
      "step": 121500
    },
    {
      "epoch": 1.6328060172916834,
      "grad_norm": 0.028657691553235054,
      "learning_rate": 1.3471184988891566e-05,
      "loss": 0.0457,
      "step": 122000
    },
    {
      "epoch": 1.6394978452314035,
      "grad_norm": 0.0075733596459031105,
      "learning_rate": 1.3444417677132686e-05,
      "loss": 0.0195,
      "step": 122500
    },
    {
      "epoch": 1.6461896731711234,
      "grad_norm": 0.02366497553884983,
      "learning_rate": 1.3417650365373806e-05,
      "loss": 0.0384,
      "step": 123000
    },
    {
      "epoch": 1.6528815011108433,
      "grad_norm": 0.1062995195388794,
      "learning_rate": 1.3390883053614927e-05,
      "loss": 0.0357,
      "step": 123500
    },
    {
      "epoch": 1.6595733290505634,
      "grad_norm": 0.1449146270751953,
      "learning_rate": 1.3364115741856047e-05,
      "loss": 0.0454,
      "step": 124000
    },
    {
      "epoch": 1.6662651569902835,
      "grad_norm": 0.01711772382259369,
      "learning_rate": 1.3337348430097166e-05,
      "loss": 0.0308,
      "step": 124500
    },
    {
      "epoch": 1.6729569849300034,
      "grad_norm": 0.015910351648926735,
      "learning_rate": 1.3310688187585321e-05,
      "loss": 0.0355,
      "step": 125000
    },
    {
      "epoch": 1.6796488128697233,
      "grad_norm": 0.020999716594815254,
      "learning_rate": 1.3283920875826441e-05,
      "loss": 0.0345,
      "step": 125500
    },
    {
      "epoch": 1.6863406408094435,
      "grad_norm": 0.05663260444998741,
      "learning_rate": 1.3257153564067561e-05,
      "loss": 0.0488,
      "step": 126000
    },
    {
      "epoch": 1.6930324687491636,
      "grad_norm": 0.017926068976521492,
      "learning_rate": 1.3230386252308682e-05,
      "loss": 0.0437,
      "step": 126500
    },
    {
      "epoch": 1.6997242966888835,
      "grad_norm": 0.03161977604031563,
      "learning_rate": 1.3203618940549802e-05,
      "loss": 0.0514,
      "step": 127000
    },
    {
      "epoch": 1.7064161246286036,
      "grad_norm": 0.026914365589618683,
      "learning_rate": 1.317685162879092e-05,
      "loss": 0.0397,
      "step": 127500
    },
    {
      "epoch": 1.7131079525683237,
      "grad_norm": 0.1037825420498848,
      "learning_rate": 1.3150084317032041e-05,
      "loss": 0.059,
      "step": 128000
    },
    {
      "epoch": 1.7197997805080436,
      "grad_norm": 0.04650367796421051,
      "learning_rate": 1.3123317005273161e-05,
      "loss": 0.0548,
      "step": 128500
    },
    {
      "epoch": 1.7264916084477635,
      "grad_norm": 0.03472224622964859,
      "learning_rate": 1.3096656762761317e-05,
      "loss": 0.0632,
      "step": 129000
    },
    {
      "epoch": 1.7331834363874836,
      "grad_norm": 0.0226438008248806,
      "learning_rate": 1.3069889451002437e-05,
      "loss": 0.0558,
      "step": 129500
    },
    {
      "epoch": 1.7398752643272037,
      "grad_norm": 0.026432180777192116,
      "learning_rate": 1.3043175673867074e-05,
      "loss": 0.0362,
      "step": 130000
    },
    {
      "epoch": 1.7465670922669236,
      "grad_norm": 0.32273781299591064,
      "learning_rate": 1.3016408362108194e-05,
      "loss": 0.1052,
      "step": 130500
    },
    {
      "epoch": 1.7532589202066435,
      "grad_norm": 0.15867729485034943,
      "learning_rate": 1.2989641050349316e-05,
      "loss": 0.1091,
      "step": 131000
    },
    {
      "epoch": 1.7599507481463637,
      "grad_norm": 0.08895464986562729,
      "learning_rate": 1.2962927273213951e-05,
      "loss": 0.0667,
      "step": 131500
    },
    {
      "epoch": 1.7666425760860838,
      "grad_norm": 0.05708571523427963,
      "learning_rate": 1.2936213496078588e-05,
      "loss": 0.0609,
      "step": 132000
    },
    {
      "epoch": 1.7733344040258037,
      "grad_norm": 0.034740228205919266,
      "learning_rate": 1.2909446184319709e-05,
      "loss": 0.0582,
      "step": 132500
    },
    {
      "epoch": 1.7800262319655236,
      "grad_norm": 0.14775212109088898,
      "learning_rate": 1.2882785941807866e-05,
      "loss": 0.0851,
      "step": 133000
    },
    {
      "epoch": 1.7867180599052437,
      "grad_norm": 0.03092982806265354,
      "learning_rate": 1.2856072164672504e-05,
      "loss": 0.0518,
      "step": 133500
    },
    {
      "epoch": 1.7934098878449638,
      "grad_norm": 0.03586046025156975,
      "learning_rate": 1.282941192216066e-05,
      "loss": 0.0668,
      "step": 134000
    },
    {
      "epoch": 1.8001017157846837,
      "grad_norm": 0.09080594778060913,
      "learning_rate": 1.2802644610401778e-05,
      "loss": 0.1488,
      "step": 134500
    },
    {
      "epoch": 1.8067935437244036,
      "grad_norm": 0.043096620589494705,
      "learning_rate": 1.2775877298642898e-05,
      "loss": 0.0901,
      "step": 135000
    },
    {
      "epoch": 1.813485371664124,
      "grad_norm": 0.05278338864445686,
      "learning_rate": 1.2749109986884019e-05,
      "loss": 0.0905,
      "step": 135500
    },
    {
      "epoch": 1.8201771996038438,
      "grad_norm": 2.250192165374756,
      "learning_rate": 1.2722342675125139e-05,
      "loss": 0.0847,
      "step": 136000
    },
    {
      "epoch": 1.8268690275435637,
      "grad_norm": 0.09124220162630081,
      "learning_rate": 1.2695575363366258e-05,
      "loss": 0.0737,
      "step": 136500
    },
    {
      "epoch": 1.8335608554832838,
      "grad_norm": 0.09324472397565842,
      "learning_rate": 1.2668808051607378e-05,
      "loss": 0.0524,
      "step": 137000
    },
    {
      "epoch": 1.840252683423004,
      "grad_norm": 0.034724656492471695,
      "learning_rate": 1.2642040739848498e-05,
      "loss": 0.0588,
      "step": 137500
    },
    {
      "epoch": 1.8469445113627239,
      "grad_norm": 0.02755643054842949,
      "learning_rate": 1.2615273428089619e-05,
      "loss": 0.0476,
      "step": 138000
    },
    {
      "epoch": 1.8536363393024438,
      "grad_norm": 0.041722849011421204,
      "learning_rate": 1.2588506116330739e-05,
      "loss": 0.0614,
      "step": 138500
    },
    {
      "epoch": 1.8603281672421639,
      "grad_norm": 0.031051188707351685,
      "learning_rate": 1.2561738804571857e-05,
      "loss": 0.06,
      "step": 139000
    },
    {
      "epoch": 1.867019995181884,
      "grad_norm": 4.243847370147705,
      "learning_rate": 1.2534971492812978e-05,
      "loss": 0.0442,
      "step": 139500
    },
    {
      "epoch": 1.873711823121604,
      "grad_norm": 0.0689779743552208,
      "learning_rate": 1.2508204181054098e-05,
      "loss": 0.0538,
      "step": 140000
    },
    {
      "epoch": 1.8804036510613238,
      "grad_norm": 136.09742736816406,
      "learning_rate": 1.2481436869295218e-05,
      "loss": 0.0508,
      "step": 140500
    },
    {
      "epoch": 1.887095479001044,
      "grad_norm": 0.055934514850378036,
      "learning_rate": 1.2454669557536339e-05,
      "loss": 0.0629,
      "step": 141000
    },
    {
      "epoch": 1.893787306940764,
      "grad_norm": 0.07401353120803833,
      "learning_rate": 1.2427902245777457e-05,
      "loss": 0.036,
      "step": 141500
    },
    {
      "epoch": 1.900479134880484,
      "grad_norm": 0.031206905841827393,
      "learning_rate": 1.2401134934018578e-05,
      "loss": 0.0592,
      "step": 142000
    },
    {
      "epoch": 1.9071709628202038,
      "grad_norm": 0.02629106119275093,
      "learning_rate": 1.2374367622259698e-05,
      "loss": 0.0441,
      "step": 142500
    },
    {
      "epoch": 1.913862790759924,
      "grad_norm": 2.8135976791381836,
      "learning_rate": 1.2347600310500818e-05,
      "loss": 0.0619,
      "step": 143000
    },
    {
      "epoch": 1.920554618699644,
      "grad_norm": 0.04473743587732315,
      "learning_rate": 1.2320832998741939e-05,
      "loss": 0.0428,
      "step": 143500
    },
    {
      "epoch": 1.927246446639364,
      "grad_norm": 0.013387288898229599,
      "learning_rate": 1.2294065686983057e-05,
      "loss": 0.0297,
      "step": 144000
    },
    {
      "epoch": 1.933938274579084,
      "grad_norm": 0.04457905516028404,
      "learning_rate": 1.2267298375224177e-05,
      "loss": 0.0385,
      "step": 144500
    },
    {
      "epoch": 1.9406301025188042,
      "grad_norm": 0.08084549754858017,
      "learning_rate": 1.2240531063465298e-05,
      "loss": 0.0376,
      "step": 145000
    },
    {
      "epoch": 1.947321930458524,
      "grad_norm": 0.00888283271342516,
      "learning_rate": 1.2213763751706418e-05,
      "loss": 0.0318,
      "step": 145500
    },
    {
      "epoch": 1.954013758398244,
      "grad_norm": 0.014209375716745853,
      "learning_rate": 1.2186996439947537e-05,
      "loss": 0.0353,
      "step": 146000
    },
    {
      "epoch": 1.960705586337964,
      "grad_norm": 0.01572902500629425,
      "learning_rate": 1.2160229128188657e-05,
      "loss": 0.0522,
      "step": 146500
    },
    {
      "epoch": 1.9673974142776842,
      "grad_norm": 0.012360530905425549,
      "learning_rate": 1.2133461816429777e-05,
      "loss": 0.0398,
      "step": 147000
    },
    {
      "epoch": 1.9740892422174041,
      "grad_norm": 0.05342497304081917,
      "learning_rate": 1.2106694504670898e-05,
      "loss": 0.0488,
      "step": 147500
    },
    {
      "epoch": 1.980781070157124,
      "grad_norm": 0.01695808582007885,
      "learning_rate": 1.2079927192912018e-05,
      "loss": 0.0436,
      "step": 148000
    },
    {
      "epoch": 1.9874728980968441,
      "grad_norm": 0.016566159203648567,
      "learning_rate": 1.2053159881153136e-05,
      "loss": 0.0376,
      "step": 148500
    },
    {
      "epoch": 1.9941647260365643,
      "grad_norm": 0.021183745935559273,
      "learning_rate": 1.2026392569394257e-05,
      "loss": 0.0405,
      "step": 149000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9701410637329693,
      "eval_loss": 0.11011311411857605,
      "eval_runtime": 141.4789,
      "eval_samples_per_second": 528.121,
      "eval_steps_per_second": 66.017,
      "step": 149436
    },
    {
      "epoch": 2.000856553976284,
      "grad_norm": 0.014743424020707607,
      "learning_rate": 1.1999625257635377e-05,
      "loss": 0.0513,
      "step": 149500
    },
    {
      "epoch": 2.007548381916004,
      "grad_norm": 0.024882322177290916,
      "learning_rate": 1.1972857945876497e-05,
      "loss": 0.033,
      "step": 150000
    },
    {
      "epoch": 2.0142402098557244,
      "grad_norm": 0.018975302577018738,
      "learning_rate": 1.1946090634117618e-05,
      "loss": 0.0332,
      "step": 150500
    },
    {
      "epoch": 2.0209320377954443,
      "grad_norm": 0.01157169695943594,
      "learning_rate": 1.1919376856982255e-05,
      "loss": 0.0452,
      "step": 151000
    },
    {
      "epoch": 2.027623865735164,
      "grad_norm": 0.1720859557390213,
      "learning_rate": 1.1892609545223375e-05,
      "loss": 0.0343,
      "step": 151500
    },
    {
      "epoch": 2.034315693674884,
      "grad_norm": 0.009877159260213375,
      "learning_rate": 1.1865842233464495e-05,
      "loss": 0.0415,
      "step": 152000
    },
    {
      "epoch": 2.0410075216146044,
      "grad_norm": 0.012580538168549538,
      "learning_rate": 1.1839074921705614e-05,
      "loss": 0.0283,
      "step": 152500
    },
    {
      "epoch": 2.0476993495543243,
      "grad_norm": 0.017940938472747803,
      "learning_rate": 1.1812307609946734e-05,
      "loss": 0.0436,
      "step": 153000
    },
    {
      "epoch": 2.054391177494044,
      "grad_norm": 0.024321354925632477,
      "learning_rate": 1.1785540298187854e-05,
      "loss": 0.0337,
      "step": 153500
    },
    {
      "epoch": 2.061083005433764,
      "grad_norm": 0.045890022069215775,
      "learning_rate": 1.1758772986428975e-05,
      "loss": 0.0418,
      "step": 154000
    },
    {
      "epoch": 2.0677748333734844,
      "grad_norm": 0.10088034719228745,
      "learning_rate": 1.1732059209293612e-05,
      "loss": 0.0334,
      "step": 154500
    },
    {
      "epoch": 2.0744666613132043,
      "grad_norm": 0.056403763592243195,
      "learning_rate": 1.1705291897534732e-05,
      "loss": 0.025,
      "step": 155000
    },
    {
      "epoch": 2.0811584892529242,
      "grad_norm": 0.014336365275084972,
      "learning_rate": 1.1678524585775852e-05,
      "loss": 0.0361,
      "step": 155500
    },
    {
      "epoch": 2.087850317192644,
      "grad_norm": 0.011972217820584774,
      "learning_rate": 1.1651757274016971e-05,
      "loss": 0.0346,
      "step": 156000
    },
    {
      "epoch": 2.0945421451323645,
      "grad_norm": 0.010162748396396637,
      "learning_rate": 1.1624989962258091e-05,
      "loss": 0.0369,
      "step": 156500
    },
    {
      "epoch": 2.1012339730720844,
      "grad_norm": 0.03974224999547005,
      "learning_rate": 1.1598222650499212e-05,
      "loss": 0.0468,
      "step": 157000
    },
    {
      "epoch": 2.1079258010118043,
      "grad_norm": 0.010212053544819355,
      "learning_rate": 1.1571455338740332e-05,
      "loss": 0.0313,
      "step": 157500
    },
    {
      "epoch": 2.1146176289515246,
      "grad_norm": 0.012156493030488491,
      "learning_rate": 1.1544688026981452e-05,
      "loss": 0.0327,
      "step": 158000
    },
    {
      "epoch": 2.1213094568912445,
      "grad_norm": 0.010129276663064957,
      "learning_rate": 1.151792071522257e-05,
      "loss": 0.0315,
      "step": 158500
    },
    {
      "epoch": 2.1280012848309644,
      "grad_norm": 0.03171919658780098,
      "learning_rate": 1.1491153403463691e-05,
      "loss": 0.0301,
      "step": 159000
    },
    {
      "epoch": 2.1346931127706843,
      "grad_norm": 0.012529107742011547,
      "learning_rate": 1.1464386091704811e-05,
      "loss": 0.0321,
      "step": 159500
    },
    {
      "epoch": 2.1413849407104046,
      "grad_norm": 0.02687712386250496,
      "learning_rate": 1.1437672314569448e-05,
      "loss": 0.0287,
      "step": 160000
    },
    {
      "epoch": 2.1480767686501245,
      "grad_norm": 0.004853082820773125,
      "learning_rate": 1.1410905002810569e-05,
      "loss": 0.0297,
      "step": 160500
    },
    {
      "epoch": 2.1547685965898444,
      "grad_norm": 0.9522340893745422,
      "learning_rate": 1.1384137691051689e-05,
      "loss": 0.04,
      "step": 161000
    },
    {
      "epoch": 2.1614604245295643,
      "grad_norm": 0.012475666590034962,
      "learning_rate": 1.135737037929281e-05,
      "loss": 0.0462,
      "step": 161500
    },
    {
      "epoch": 2.1681522524692847,
      "grad_norm": 0.04508400335907936,
      "learning_rate": 1.133060306753393e-05,
      "loss": 0.0384,
      "step": 162000
    },
    {
      "epoch": 2.1748440804090046,
      "grad_norm": 0.034475456923246384,
      "learning_rate": 1.1303835755775048e-05,
      "loss": 0.0296,
      "step": 162500
    },
    {
      "epoch": 2.1815359083487245,
      "grad_norm": 0.008704333566129208,
      "learning_rate": 1.1277068444016168e-05,
      "loss": 0.0313,
      "step": 163000
    },
    {
      "epoch": 2.1882277362884444,
      "grad_norm": 0.009192476980388165,
      "learning_rate": 1.1250354666880805e-05,
      "loss": 0.0252,
      "step": 163500
    },
    {
      "epoch": 2.1949195642281647,
      "grad_norm": 0.010200354270637035,
      "learning_rate": 1.1223587355121926e-05,
      "loss": 0.0304,
      "step": 164000
    },
    {
      "epoch": 2.2016113921678846,
      "grad_norm": 0.00759513396769762,
      "learning_rate": 1.1196820043363046e-05,
      "loss": 0.0247,
      "step": 164500
    },
    {
      "epoch": 2.2083032201076045,
      "grad_norm": 0.007461327128112316,
      "learning_rate": 1.1170052731604166e-05,
      "loss": 0.022,
      "step": 165000
    },
    {
      "epoch": 2.2149950480473244,
      "grad_norm": 0.012259874492883682,
      "learning_rate": 1.1143285419845287e-05,
      "loss": 0.0273,
      "step": 165500
    },
    {
      "epoch": 2.2216868759870447,
      "grad_norm": 0.027161134406924248,
      "learning_rate": 1.1116518108086405e-05,
      "loss": 0.027,
      "step": 166000
    },
    {
      "epoch": 2.2283787039267646,
      "grad_norm": 0.010672552511096,
      "learning_rate": 1.1089750796327526e-05,
      "loss": 0.0254,
      "step": 166500
    },
    {
      "epoch": 2.2350705318664845,
      "grad_norm": 0.05021282285451889,
      "learning_rate": 1.1062983484568646e-05,
      "loss": 0.0337,
      "step": 167000
    },
    {
      "epoch": 2.241762359806205,
      "grad_norm": 0.015240469947457314,
      "learning_rate": 1.1036216172809766e-05,
      "loss": 0.0278,
      "step": 167500
    },
    {
      "epoch": 2.2484541877459248,
      "grad_norm": 0.030986929312348366,
      "learning_rate": 1.1009448861050886e-05,
      "loss": 0.0343,
      "step": 168000
    },
    {
      "epoch": 2.2551460156856447,
      "grad_norm": 3.5666282176971436,
      "learning_rate": 1.0982735083915523e-05,
      "loss": 0.0591,
      "step": 168500
    },
    {
      "epoch": 2.2618378436253646,
      "grad_norm": 0.045969266444444656,
      "learning_rate": 1.0956074841403679e-05,
      "loss": 0.0405,
      "step": 169000
    },
    {
      "epoch": 2.268529671565085,
      "grad_norm": 0.021529002115130424,
      "learning_rate": 1.0929414598891834e-05,
      "loss": 0.0277,
      "step": 169500
    },
    {
      "epoch": 2.275221499504805,
      "grad_norm": 0.03490947186946869,
      "learning_rate": 1.0902647287132954e-05,
      "loss": 0.0305,
      "step": 170000
    },
    {
      "epoch": 2.2819133274445247,
      "grad_norm": 0.055842459201812744,
      "learning_rate": 1.087598704462111e-05,
      "loss": 0.0661,
      "step": 170500
    },
    {
      "epoch": 2.2886051553842446,
      "grad_norm": 0.21944259107112885,
      "learning_rate": 1.084921973286223e-05,
      "loss": 0.0745,
      "step": 171000
    },
    {
      "epoch": 2.295296983323965,
      "grad_norm": 0.29975956678390503,
      "learning_rate": 1.0822452421103348e-05,
      "loss": 0.1481,
      "step": 171500
    },
    {
      "epoch": 2.301988811263685,
      "grad_norm": 0.17759977281093597,
      "learning_rate": 1.0795685109344469e-05,
      "loss": 0.1228,
      "step": 172000
    },
    {
      "epoch": 2.3086806392034047,
      "grad_norm": 0.2836230993270874,
      "learning_rate": 1.0768917797585589e-05,
      "loss": 0.1639,
      "step": 172500
    },
    {
      "epoch": 2.315372467143125,
      "grad_norm": 0.29546138644218445,
      "learning_rate": 1.074215048582671e-05,
      "loss": 0.1602,
      "step": 173000
    },
    {
      "epoch": 2.322064295082845,
      "grad_norm": 0.22566671669483185,
      "learning_rate": 1.0715383174067828e-05,
      "loss": 0.1367,
      "step": 173500
    },
    {
      "epoch": 2.328756123022565,
      "grad_norm": 0.2433224618434906,
      "learning_rate": 1.0688615862308948e-05,
      "loss": 0.1432,
      "step": 174000
    },
    {
      "epoch": 2.3354479509622847,
      "grad_norm": 0.2720945477485657,
      "learning_rate": 1.0661848550550069e-05,
      "loss": 0.1504,
      "step": 174500
    },
    {
      "epoch": 2.3421397789020046,
      "grad_norm": 0.20214059948921204,
      "learning_rate": 1.0635081238791189e-05,
      "loss": 0.1308,
      "step": 175000
    },
    {
      "epoch": 2.348831606841725,
      "grad_norm": 0.19368499517440796,
      "learning_rate": 1.060831392703231e-05,
      "loss": 0.1551,
      "step": 175500
    },
    {
      "epoch": 2.355523434781445,
      "grad_norm": 0.08483288437128067,
      "learning_rate": 1.0581546615273428e-05,
      "loss": 0.1011,
      "step": 176000
    },
    {
      "epoch": 2.3622152627211648,
      "grad_norm": 3.920051336288452,
      "learning_rate": 1.0554779303514548e-05,
      "loss": 0.0445,
      "step": 176500
    },
    {
      "epoch": 2.368907090660885,
      "grad_norm": 0.14078451693058014,
      "learning_rate": 1.0528011991755668e-05,
      "loss": 0.0885,
      "step": 177000
    },
    {
      "epoch": 2.375598918600605,
      "grad_norm": 0.06469178199768066,
      "learning_rate": 1.0501244679996789e-05,
      "loss": 0.1074,
      "step": 177500
    },
    {
      "epoch": 2.382290746540325,
      "grad_norm": 0.03259502351284027,
      "learning_rate": 1.0474477368237909e-05,
      "loss": 0.0622,
      "step": 178000
    },
    {
      "epoch": 2.388982574480045,
      "grad_norm": 0.03893551975488663,
      "learning_rate": 1.0447710056479028e-05,
      "loss": 0.0676,
      "step": 178500
    },
    {
      "epoch": 2.395674402419765,
      "grad_norm": 0.033625103533267975,
      "learning_rate": 1.0420942744720148e-05,
      "loss": 0.0539,
      "step": 179000
    },
    {
      "epoch": 2.402366230359485,
      "grad_norm": 0.031288646161556244,
      "learning_rate": 1.0394175432961268e-05,
      "loss": 0.0406,
      "step": 179500
    },
    {
      "epoch": 2.409058058299205,
      "grad_norm": 0.06309214979410172,
      "learning_rate": 1.0367408121202389e-05,
      "loss": 0.0703,
      "step": 180000
    },
    {
      "epoch": 2.415749886238925,
      "grad_norm": 0.059222616255283356,
      "learning_rate": 1.0340640809443507e-05,
      "loss": 0.0635,
      "step": 180500
    },
    {
      "epoch": 2.422441714178645,
      "grad_norm": 0.06361386924982071,
      "learning_rate": 1.0313873497684627e-05,
      "loss": 0.0688,
      "step": 181000
    },
    {
      "epoch": 2.429133542118365,
      "grad_norm": 0.06038971617817879,
      "learning_rate": 1.0287213255172783e-05,
      "loss": 0.0615,
      "step": 181500
    },
    {
      "epoch": 2.435825370058085,
      "grad_norm": 0.022834917530417442,
      "learning_rate": 1.0260445943413903e-05,
      "loss": 0.0798,
      "step": 182000
    },
    {
      "epoch": 2.4425171979978053,
      "grad_norm": 0.03711283951997757,
      "learning_rate": 1.0233678631655023e-05,
      "loss": 0.0558,
      "step": 182500
    },
    {
      "epoch": 2.449209025937525,
      "grad_norm": 0.020179690793156624,
      "learning_rate": 1.020696485451966e-05,
      "loss": 0.0485,
      "step": 183000
    },
    {
      "epoch": 2.455900853877245,
      "grad_norm": 3.8645541667938232,
      "learning_rate": 1.018019754276078e-05,
      "loss": 0.043,
      "step": 183500
    },
    {
      "epoch": 2.462592681816965,
      "grad_norm": 4.103487968444824,
      "learning_rate": 1.0153430231001901e-05,
      "loss": 0.0721,
      "step": 184000
    },
    {
      "epoch": 2.469284509756685,
      "grad_norm": 0.0207922775298357,
      "learning_rate": 1.0126716453866538e-05,
      "loss": 0.0481,
      "step": 184500
    },
    {
      "epoch": 2.4759763376964052,
      "grad_norm": 0.024259628728032112,
      "learning_rate": 1.009994914210766e-05,
      "loss": 0.0495,
      "step": 185000
    },
    {
      "epoch": 2.482668165636125,
      "grad_norm": 3.916311025619507,
      "learning_rate": 1.007318183034878e-05,
      "loss": 0.0458,
      "step": 185500
    },
    {
      "epoch": 2.489359993575845,
      "grad_norm": 0.020015699788928032,
      "learning_rate": 1.00464145185899e-05,
      "loss": 0.0339,
      "step": 186000
    },
    {
      "epoch": 2.4960518215155654,
      "grad_norm": 0.10338233411312103,
      "learning_rate": 1.0019647206831017e-05,
      "loss": 0.0572,
      "step": 186500
    },
    {
      "epoch": 2.5027436494552853,
      "grad_norm": 0.04718777537345886,
      "learning_rate": 9.992879895072138e-06,
      "loss": 0.0741,
      "step": 187000
    },
    {
      "epoch": 2.509435477395005,
      "grad_norm": 3.9127631187438965,
      "learning_rate": 9.96611258331326e-06,
      "loss": 0.1127,
      "step": 187500
    },
    {
      "epoch": 2.5161273053347255,
      "grad_norm": 0.293169766664505,
      "learning_rate": 9.939345271554378e-06,
      "loss": 0.1491,
      "step": 188000
    },
    {
      "epoch": 2.5228191332744454,
      "grad_norm": 0.19296787679195404,
      "learning_rate": 9.912577959795499e-06,
      "loss": 0.1488,
      "step": 188500
    },
    {
      "epoch": 2.5295109612141653,
      "grad_norm": 0.25319308042526245,
      "learning_rate": 9.885810648036619e-06,
      "loss": 0.1421,
      "step": 189000
    },
    {
      "epoch": 2.536202789153885,
      "grad_norm": 0.18185019493103027,
      "learning_rate": 9.859043336277737e-06,
      "loss": 0.1303,
      "step": 189500
    },
    {
      "epoch": 2.542894617093605,
      "grad_norm": 0.17656481266021729,
      "learning_rate": 9.83227602451886e-06,
      "loss": 0.1381,
      "step": 190000
    },
    {
      "epoch": 2.5495864450333254,
      "grad_norm": 3.623969316482544,
      "learning_rate": 9.805508712759978e-06,
      "loss": 0.1518,
      "step": 190500
    },
    {
      "epoch": 2.5562782729730453,
      "grad_norm": 0.2261667400598526,
      "learning_rate": 9.778741401001098e-06,
      "loss": 0.1428,
      "step": 191000
    },
    {
      "epoch": 2.5629701009127652,
      "grad_norm": 7.004210472106934,
      "learning_rate": 9.751974089242217e-06,
      "loss": 0.176,
      "step": 191500
    },
    {
      "epoch": 2.5696619288524856,
      "grad_norm": 0.31164732575416565,
      "learning_rate": 9.725206777483337e-06,
      "loss": 0.1394,
      "step": 192000
    },
    {
      "epoch": 2.5763537567922055,
      "grad_norm": 0.21044465899467468,
      "learning_rate": 9.69843946572446e-06,
      "loss": 0.1501,
      "step": 192500
    },
    {
      "epoch": 2.5830455847319254,
      "grad_norm": 3.6177120208740234,
      "learning_rate": 9.671672153965578e-06,
      "loss": 0.1478,
      "step": 193000
    },
    {
      "epoch": 2.5897374126716453,
      "grad_norm": 0.25991302728652954,
      "learning_rate": 9.644904842206698e-06,
      "loss": 0.1519,
      "step": 193500
    },
    {
      "epoch": 2.596429240611365,
      "grad_norm": 3.5384180545806885,
      "learning_rate": 9.618137530447817e-06,
      "loss": 0.1387,
      "step": 194000
    },
    {
      "epoch": 2.6031210685510855,
      "grad_norm": 0.17788855731487274,
      "learning_rate": 9.591370218688937e-06,
      "loss": 0.1354,
      "step": 194500
    },
    {
      "epoch": 2.6098128964908054,
      "grad_norm": 0.2583763003349304,
      "learning_rate": 9.564602906930059e-06,
      "loss": 0.1396,
      "step": 195000
    },
    {
      "epoch": 2.6165047244305253,
      "grad_norm": 0.2659458816051483,
      "learning_rate": 9.537835595171178e-06,
      "loss": 0.1589,
      "step": 195500
    },
    {
      "epoch": 2.6231965523702456,
      "grad_norm": 0.22650808095932007,
      "learning_rate": 9.511068283412298e-06,
      "loss": 0.1504,
      "step": 196000
    },
    {
      "epoch": 2.6298883803099655,
      "grad_norm": 0.25104376673698425,
      "learning_rate": 9.484300971653417e-06,
      "loss": 0.145,
      "step": 196500
    },
    {
      "epoch": 2.6365802082496854,
      "grad_norm": 6.957966327667236,
      "learning_rate": 9.457533659894537e-06,
      "loss": 0.1607,
      "step": 197000
    },
    {
      "epoch": 2.6432720361894058,
      "grad_norm": 0.3011155128479004,
      "learning_rate": 9.430766348135657e-06,
      "loss": 0.1449,
      "step": 197500
    },
    {
      "epoch": 2.6499638641291257,
      "grad_norm": 0.25950393080711365,
      "learning_rate": 9.403999036376778e-06,
      "loss": 0.1246,
      "step": 198000
    },
    {
      "epoch": 2.6566556920688456,
      "grad_norm": 3.531324863433838,
      "learning_rate": 9.377231724617898e-06,
      "loss": 0.1516,
      "step": 198500
    },
    {
      "epoch": 2.6633475200085654,
      "grad_norm": 0.2319655567407608,
      "learning_rate": 9.350464412859017e-06,
      "loss": 0.1507,
      "step": 199000
    },
    {
      "epoch": 2.6700393479482853,
      "grad_norm": 0.3069227933883667,
      "learning_rate": 9.323697101100137e-06,
      "loss": 0.1442,
      "step": 199500
    },
    {
      "epoch": 2.6767311758880057,
      "grad_norm": 0.2615123391151428,
      "learning_rate": 9.296929789341257e-06,
      "loss": 0.1443,
      "step": 200000
    },
    {
      "epoch": 2.6834230038277256,
      "grad_norm": 0.23294973373413086,
      "learning_rate": 9.270162477582377e-06,
      "loss": 0.1396,
      "step": 200500
    },
    {
      "epoch": 2.6901148317674455,
      "grad_norm": 3.4352779388427734,
      "learning_rate": 9.243395165823496e-06,
      "loss": 0.1572,
      "step": 201000
    },
    {
      "epoch": 2.696806659707166,
      "grad_norm": 0.29786938428878784,
      "learning_rate": 9.216627854064616e-06,
      "loss": 0.1446,
      "step": 201500
    },
    {
      "epoch": 2.7034984876468857,
      "grad_norm": 3.5711209774017334,
      "learning_rate": 9.189860542305737e-06,
      "loss": 0.138,
      "step": 202000
    },
    {
      "epoch": 2.7101903155866056,
      "grad_norm": 0.29625627398490906,
      "learning_rate": 9.163093230546857e-06,
      "loss": 0.1391,
      "step": 202500
    },
    {
      "epoch": 2.7168821435263255,
      "grad_norm": 0.2363283783197403,
      "learning_rate": 9.136325918787977e-06,
      "loss": 0.1416,
      "step": 203000
    },
    {
      "epoch": 2.7235739714660454,
      "grad_norm": 0.24903246760368347,
      "learning_rate": 9.109558607029096e-06,
      "loss": 0.1484,
      "step": 203500
    },
    {
      "epoch": 2.7302657994057657,
      "grad_norm": 3.4251129627227783,
      "learning_rate": 9.082791295270216e-06,
      "loss": 0.1596,
      "step": 204000
    },
    {
      "epoch": 2.7369576273454856,
      "grad_norm": 0.23614564538002014,
      "learning_rate": 9.056023983511336e-06,
      "loss": 0.1356,
      "step": 204500
    },
    {
      "epoch": 2.7436494552852055,
      "grad_norm": 3.3796863555908203,
      "learning_rate": 9.029256671752457e-06,
      "loss": 0.1574,
      "step": 205000
    },
    {
      "epoch": 2.750341283224926,
      "grad_norm": 0.21389435231685638,
      "learning_rate": 9.002542894617094e-06,
      "loss": 0.1387,
      "step": 205500
    },
    {
      "epoch": 2.7570331111646458,
      "grad_norm": 0.22956892848014832,
      "learning_rate": 8.975775582858214e-06,
      "loss": 0.146,
      "step": 206000
    },
    {
      "epoch": 2.7637249391043657,
      "grad_norm": 0.22299900650978088,
      "learning_rate": 8.949008271099334e-06,
      "loss": 0.1377,
      "step": 206500
    },
    {
      "epoch": 2.770416767044086,
      "grad_norm": 0.2553170323371887,
      "learning_rate": 8.922240959340455e-06,
      "loss": 0.1356,
      "step": 207000
    },
    {
      "epoch": 2.777108594983806,
      "grad_norm": 3.4568071365356445,
      "learning_rate": 8.895473647581573e-06,
      "loss": 0.1377,
      "step": 207500
    },
    {
      "epoch": 2.783800422923526,
      "grad_norm": 0.21775321662425995,
      "learning_rate": 8.868706335822694e-06,
      "loss": 0.1181,
      "step": 208000
    },
    {
      "epoch": 2.7904922508632457,
      "grad_norm": 0.26028358936309814,
      "learning_rate": 8.841939024063814e-06,
      "loss": 0.159,
      "step": 208500
    },
    {
      "epoch": 2.7971840788029656,
      "grad_norm": 0.21942174434661865,
      "learning_rate": 8.815171712304934e-06,
      "loss": 0.1292,
      "step": 209000
    },
    {
      "epoch": 2.803875906742686,
      "grad_norm": 0.19185896217823029,
      "learning_rate": 8.788404400546053e-06,
      "loss": 0.1347,
      "step": 209500
    },
    {
      "epoch": 2.810567734682406,
      "grad_norm": 0.215616837143898,
      "learning_rate": 8.761637088787173e-06,
      "loss": 0.1414,
      "step": 210000
    },
    {
      "epoch": 2.8172595626221257,
      "grad_norm": 0.3563590943813324,
      "learning_rate": 8.734869777028293e-06,
      "loss": 0.1519,
      "step": 210500
    },
    {
      "epoch": 2.823951390561846,
      "grad_norm": 0.28706836700439453,
      "learning_rate": 8.708102465269414e-06,
      "loss": 0.1342,
      "step": 211000
    },
    {
      "epoch": 2.830643218501566,
      "grad_norm": 0.3002801537513733,
      "learning_rate": 8.68138868813405e-06,
      "loss": 0.1642,
      "step": 211500
    },
    {
      "epoch": 2.837335046441286,
      "grad_norm": 0.2509180009365082,
      "learning_rate": 8.654621376375173e-06,
      "loss": 0.1366,
      "step": 212000
    },
    {
      "epoch": 2.8440268743810058,
      "grad_norm": 0.3104604184627533,
      "learning_rate": 8.627854064616291e-06,
      "loss": 0.1557,
      "step": 212500
    },
    {
      "epoch": 2.8507187023207257,
      "grad_norm": 0.37855932116508484,
      "learning_rate": 8.601086752857412e-06,
      "loss": 0.1617,
      "step": 213000
    },
    {
      "epoch": 2.857410530260446,
      "grad_norm": 0.25462913513183594,
      "learning_rate": 8.574372975722048e-06,
      "loss": 0.1452,
      "step": 213500
    },
    {
      "epoch": 2.864102358200166,
      "grad_norm": 0.23478612303733826,
      "learning_rate": 8.547605663963169e-06,
      "loss": 0.1466,
      "step": 214000
    },
    {
      "epoch": 2.870794186139886,
      "grad_norm": 0.2542835474014282,
      "learning_rate": 8.520838352204289e-06,
      "loss": 0.1379,
      "step": 214500
    },
    {
      "epoch": 2.877486014079606,
      "grad_norm": 0.31579676270484924,
      "learning_rate": 8.494071040445408e-06,
      "loss": 0.1472,
      "step": 215000
    },
    {
      "epoch": 2.884177842019326,
      "grad_norm": 0.28162333369255066,
      "learning_rate": 8.467357263310046e-06,
      "loss": 0.1522,
      "step": 215500
    },
    {
      "epoch": 2.890869669959046,
      "grad_norm": 0.2231174111366272,
      "learning_rate": 8.440589951551167e-06,
      "loss": 0.1438,
      "step": 216000
    },
    {
      "epoch": 2.8975614978987663,
      "grad_norm": 0.27823734283447266,
      "learning_rate": 8.413822639792287e-06,
      "loss": 0.151,
      "step": 216500
    },
    {
      "epoch": 2.904253325838486,
      "grad_norm": 0.2443154901266098,
      "learning_rate": 8.387055328033407e-06,
      "loss": 0.144,
      "step": 217000
    },
    {
      "epoch": 2.910945153778206,
      "grad_norm": 3.356274127960205,
      "learning_rate": 8.360341550898044e-06,
      "loss": 0.1532,
      "step": 217500
    },
    {
      "epoch": 2.917636981717926,
      "grad_norm": 0.2370845526456833,
      "learning_rate": 8.333574239139165e-06,
      "loss": 0.1412,
      "step": 218000
    },
    {
      "epoch": 2.924328809657646,
      "grad_norm": 0.2625332176685333,
      "learning_rate": 8.306806927380285e-06,
      "loss": 0.1549,
      "step": 218500
    },
    {
      "epoch": 2.931020637597366,
      "grad_norm": 0.2539813220500946,
      "learning_rate": 8.280039615621403e-06,
      "loss": 0.1334,
      "step": 219000
    },
    {
      "epoch": 2.937712465537086,
      "grad_norm": 0.24134287238121033,
      "learning_rate": 8.253325838486042e-06,
      "loss": 0.1517,
      "step": 219500
    },
    {
      "epoch": 2.944404293476806,
      "grad_norm": 0.23712995648384094,
      "learning_rate": 8.226558526727162e-06,
      "loss": 0.1327,
      "step": 220000
    },
    {
      "epoch": 2.9510961214165263,
      "grad_norm": 0.2650874853134155,
      "learning_rate": 8.199791214968281e-06,
      "loss": 0.1449,
      "step": 220500
    },
    {
      "epoch": 2.9577879493562462,
      "grad_norm": 0.18360911309719086,
      "learning_rate": 8.173023903209401e-06,
      "loss": 0.1134,
      "step": 221000
    },
    {
      "epoch": 2.964479777295966,
      "grad_norm": 0.2463642656803131,
      "learning_rate": 8.14631012607404e-06,
      "loss": 0.1355,
      "step": 221500
    },
    {
      "epoch": 2.971171605235686,
      "grad_norm": 0.23805022239685059,
      "learning_rate": 8.119542814315159e-06,
      "loss": 0.1368,
      "step": 222000
    },
    {
      "epoch": 2.9778634331754064,
      "grad_norm": 3.3123483657836914,
      "learning_rate": 8.092775502556279e-06,
      "loss": 0.1443,
      "step": 222500
    },
    {
      "epoch": 2.9845552611151263,
      "grad_norm": 0.2458658069372177,
      "learning_rate": 8.066008190797399e-06,
      "loss": 0.1387,
      "step": 223000
    },
    {
      "epoch": 2.991247089054846,
      "grad_norm": 0.2326357513666153,
      "learning_rate": 8.039294413662036e-06,
      "loss": 0.1446,
      "step": 223500
    },
    {
      "epoch": 2.997938916994566,
      "grad_norm": 0.23355647921562195,
      "learning_rate": 8.012527101903156e-06,
      "loss": 0.1489,
      "step": 224000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9714258946973955,
      "eval_loss": 0.14278201758861542,
      "eval_runtime": 140.3204,
      "eval_samples_per_second": 532.481,
      "eval_steps_per_second": 66.562,
      "step": 224154
    }
  ],
  "logging_steps": 500,
  "max_steps": 373590,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 2
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.358659672441256e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
